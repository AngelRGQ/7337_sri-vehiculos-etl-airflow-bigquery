{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2H9Ud2ZYJOu_"
      },
      "outputs": [],
      "source": [
        "# INSTRUCCIONES DE INSTALACIÓN PARA GOOGLE COLAB:\n",
        "\"\"\"\n",
        "# Ejecutar en celdas separadas:\n",
        "\n",
        "# Celda 1: Instalación de librerías\n",
        "!pip install pandas openpyxl google-cloud-bigquery google-cloud-storage\n",
        "\n",
        "# Celda 2: Subir archivo de credenciales (opcional para simulación)\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Solo ejecutar si tienes credenciales reales\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = filename\n",
        "\n",
        "# Celda 3: Ejecutar este código completo\n",
        "\"\"\"\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "from io import StringIO\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 2: Configuración inicial\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Subir el archivo JSON de la service account\n",
        "print(\"Sube tu archivo JSON de Service Account:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Configurar variable de entorno\n",
        "for filename in uploaded.keys():\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = filename\n",
        "    print(f\"Configurado: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "P1JwnpzZU0c7",
        "outputId": "fad16ed3-9629-44eb-fa04-e3b74c40cc41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sube tu archivo JSON de Service Account:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1098044d-644f-44e5-8d11-d22d05b745b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1098044d-644f-44e5-8d11-d22d05b745b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sri-vehiculos-etl-6e14a22d7578.json to sri-vehiculos-etl-6e14a22d7578 (1).json\n",
            "Configurado: sri-vehiculos-etl-6e14a22d7578 (1).json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONFIGURACIÓN Y CLASES MOCK\n",
        "# ============================================================================\n",
        "\n",
        "# Variables de configuración\n",
        "PROJECT_ID = 'sri-vehiculos-etl'\n",
        "DATASET_ID = 'sri_vehiculos_dw'\n",
        "BUCKET_NAME = 'sri-vehiculos-etl-bucket-angel'\n",
        "\n",
        "print(f\"Configuración:\")\n",
        "print(f\"- Proyecto: {PROJECT_ID}\")\n",
        "print(f\"- Dataset: {DATASET_ID}\")\n",
        "print(f\"- Bucket: {BUCKET_NAME}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHgMk0RjK7r8",
        "outputId": "939615ea-08c5-4853-d8f0-ed77023dccaf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuración:\n",
            "- Proyecto: sri-vehiculos-etl\n",
            "- Dataset: sri_vehiculos_dw\n",
            "- Bucket: sri-vehiculos-etl-bucket-angel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 4: Crear dataset en BigQuery\n",
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Crear dataset si no existe\n",
        "dataset_ref = client.dataset(DATASET_ID)\n",
        "try:\n",
        "    client.get_dataset(dataset_ref)\n",
        "    print(f\"✅ Dataset {DATASET_ID} ya existe\")\n",
        "except:\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"US\"  # o \"EU\" según tu preferencia\n",
        "    dataset = client.create_dataset(dataset)\n",
        "    print(f\"✅ Dataset {DATASET_ID} creado exitosamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzoz9SlcLV-8",
        "outputId": "2e922a6f-565e-4f8b-ba2c-b4c096728406"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset sri_vehiculos_dw ya existe\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Celda 5: Crear bucket de Cloud Storage\n",
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client()\n",
        "BUCKET_NAME = \"sri-vehiculos-etl-bucket-angel\"\n",
        "\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "# Subimos un archivo de prueba\n",
        "blob = bucket.blob(\"raw-data/.keep\")\n",
        "blob.upload_from_string('')\n",
        "print(\"✅ Carpeta raw-data/ creada\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-asDS-QXLrnW",
        "outputId": "670349fa-629e-46c3-9aa7-c422ad64966b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Carpeta raw-data/ creada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 6: Subir archivo de datos SRI\n",
        "from google.colab import files\n",
        "from google.cloud import storage\n",
        "import io\n",
        "\n",
        "print(\"Sube tu archivo CSV de datos del SRI:\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Subir al bucket de Cloud Storage\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "for filename, content in uploaded_files.items():\n",
        "    # Subir a la carpeta raw-data\n",
        "    blob = bucket.blob(f'raw-data/sri_vehiculos.csv')\n",
        "    blob.upload_from_string(content.decode('utf-8'))\n",
        "    print(f\"✅ Archivo {filename} subido como sri_vehiculos.csv\")\n",
        "\n",
        "    # Mostrar información del archivo\n",
        "    print(f\"📊 Tamaño del archivo: {len(content)} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "JCRLnBjRYLIW",
        "outputId": "98e97ef3-4f00-4013-9ea8-2b3c5200b44b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sube tu archivo CSV de datos del SRI:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-32fcf6ac-c8e2-43fe-960d-26fa252b83cc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-32fcf6ac-c8e2-43fe-960d-26fa252b83cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving SRI-vehiculos - SRI_Vehiculos_Nuevos.csv to SRI-vehiculos - SRI_Vehiculos_Nuevos.csv\n",
            "✅ Archivo SRI-vehiculos - SRI_Vehiculos_Nuevos.csv subido como sri_vehiculos.csv\n",
            "📊 Tamaño del archivo: 75189919 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 7: Verificar configuración\n",
        "from google.cloud import bigquery, storage\n",
        "import pandas as pd\n",
        "\n",
        "print(\"🔍 Verificando configuración...\")\n",
        "\n",
        "# Verificar BigQuery\n",
        "try:\n",
        "    bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "    datasets = list(bq_client.list_datasets())\n",
        "    print(f\"✅ BigQuery conectado. Datasets disponibles: {len(datasets)}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error BigQuery: {e}\")\n",
        "\n",
        "# Verificar Cloud Storage\n",
        "try:\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(BUCKET_NAME)\n",
        "    blobs = list(bucket.list_blobs(prefix='raw-data/'))\n",
        "    print(f\"✅ Cloud Storage conectado. Archivos en raw-data/: {len(blobs)}\")\n",
        "    for blob in blobs:\n",
        "        print(f\"   - {blob.name}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error Cloud Storage: {e}\")\n",
        "\n",
        "# Verificar archivo CSV\n",
        "try:\n",
        "    blob = bucket.blob('raw-data/sri_vehiculos.csv')\n",
        "    content = blob.download_as_text()\n",
        "    df = pd.read_csv(io.StringIO(content))\n",
        "    print(f\"✅ Archivo CSV leído correctamente\")\n",
        "    print(f\"   - Filas: {len(df)}\")\n",
        "    print(f\"   - Columnas: {len(df.columns)}\")\n",
        "    print(f\"   - Columnas disponibles: {list(df.columns[:10])}\")  # Primeras 10\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error leyendo CSV: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oiVq7lhaWiV",
        "outputId": "dd66a38d-6587-44ec-f1b5-e40b8c53861f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Verificando configuración...\n",
            "✅ BigQuery conectado. Datasets disponibles: 1\n",
            "✅ Cloud Storage conectado. Archivos en raw-data/: 4\n",
            "   - raw-data/\n",
            "   - raw-data/.keep\n",
            "   - raw-data/SRI-vehiculos.xlsx\n",
            "   - raw-data/sri_vehiculos.csv\n",
            "✅ Archivo CSV leído correctamente\n",
            "   - Filas: 460550\n",
            "   - Columnas: 20\n",
            "   - Columnas disponibles: ['CATEGORÍA', 'CÓDIGO DE VEHÍCULO', 'TIPO TRANSACCIÓN', 'MARCA', 'MODELO', 'PAÍS', 'AÑO MODELO', 'CLASE', 'SUB CLASE', 'TIPO']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solución sin reinstalar ni reiniciar - Parche directo al módulo\n",
        "import sys\n",
        "import pendulum\n",
        "\n",
        "# Crear el método que Airflow espera encontrar\n",
        "if not hasattr(pendulum.tz, 'timezone') or not callable(pendulum.tz.timezone):\n",
        "    # Parche: crear la función que Airflow 2.7.0 busca\n",
        "    def timezone_function(tz_name):\n",
        "        return pendulum.timezone(tz_name)\n",
        "\n",
        "    # Aplicar el parche\n",
        "    pendulum.tz.timezone = timezone_function\n",
        "    print(\"✅ Parche aplicado a pendulum.tz.timezone\")\n",
        "\n",
        "# También parchear el módulo ya cargado en sys.modules si existe\n",
        "if 'pendulum.tz' in sys.modules:\n",
        "    sys.modules['pendulum.tz'].timezone = timezone_function\n",
        "    print(\"✅ Parche aplicado al módulo cargado\")\n",
        "\n",
        "# Limpiar solo los módulos de airflow (no pendulum)\n",
        "airflow_modules = [mod for mod in sys.modules if 'airflow' in mod]\n",
        "for mod in airflow_modules:\n",
        "    del sys.modules[mod]\n",
        "print(f\"✅ {len(airflow_modules)} módulos de airflow limpiados\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtwaMUIWgPvX",
        "outputId": "896b046b-6627-4113-9d33-5af0fc1643fd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Parche aplicado a pendulum.tz.timezone\n",
            "✅ Parche aplicado al módulo cargado\n",
            "✅ 42 módulos de airflow limpiados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLUCIÓN DEFINITIVA - Deshabilitar BD de Airflow completamente\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Variables de entorno para deshabilitar funciones que requieren BD\n",
        "os.environ['AIRFLOW__CORE__UNIT_TEST_MODE'] = 'True'\n",
        "os.environ['AIRFLOW__CORE__LOAD_EXAMPLES'] = 'False'\n",
        "os.environ['AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS'] = 'False'\n",
        "os.environ['AIRFLOW__CORE__SQL_ALCHEMY_CONN'] = 'sqlite:///:memory:'\n",
        "os.environ['AIRFLOW__WEBSERVER__EXPOSE_CONFIG'] = 'False'\n",
        "\n",
        "# Configurar logging para suprimir errores de BD\n",
        "import logging\n",
        "logging.getLogger('airflow').setLevel(logging.CRITICAL)\n",
        "logging.getLogger('sqlalchemy').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Parche para evitar queries a task_instance\n",
        "from unittest.mock import patch, MagicMock\n",
        "\n",
        "# Mock del DAG para evitar queries a BD\n",
        "original_dag_init = None\n",
        "\n",
        "def mock_dag_init(self, *args, **kwargs):\n",
        "    # Llamar constructor original pero sin validaciones de BD\n",
        "    try:\n",
        "        if original_dag_init:\n",
        "            original_dag_init(self, *args, **kwargs)\n",
        "    except:\n",
        "        # Si falla, crear DAG mínimo\n",
        "        self.dag_id = kwargs.get('dag_id', args[0] if args else 'default')\n",
        "        self.default_args = kwargs.get('default_args', {})\n",
        "        self.tasks = []\n",
        "        self.task_dict = {}\n",
        "        self._task_group = None\n",
        "\n",
        "# Importar y patchear\n",
        "from airflow import DAG as OriginalDAG\n",
        "original_dag_init = OriginalDAG.__init__\n",
        "\n",
        "# Aplicar patch\n",
        "OriginalDAG.__init__ = mock_dag_init\n",
        "\n",
        "print(\"✅ Airflow configurado para modo desarrollo sin BD\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiEnLeC7hSx9",
        "outputId": "f8547b2e-bb7c-41b5-ad3b-37b07703762e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Airflow configurado para modo desarrollo sin BD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLUCIÓN DEFINITIVA: Airflow Mock para Google Colab\n",
        "# Sin dependencias de base de datos, completamente funcional para desarrollo\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from google.cloud import storage, bigquery\n",
        "import hashlib\n",
        "from io import StringIO\n",
        "import logging\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MockDAG:\n",
        "    \"\"\"Simulador de DAG de Airflow sin dependencias de BD\"\"\"\n",
        "\n",
        "    def __init__(self, dag_id, default_args=None, description=None,\n",
        "                 schedule_interval=None, start_date=None, catchup=False,\n",
        "                 tags=None, **kwargs):\n",
        "        self.dag_id = dag_id\n",
        "        self.default_args = default_args or {}\n",
        "        self.description = description\n",
        "        self.schedule_interval = schedule_interval\n",
        "        self.start_date = start_date\n",
        "        self.catchup = catchup\n",
        "        self.tags = tags or []\n",
        "        self.tasks = []\n",
        "        self.task_dict = {}\n",
        "        self._current_dag = None\n",
        "\n",
        "        logger.info(f\"🔧 DAG '{dag_id}' creado exitosamente\")\n",
        "\n",
        "    def __enter__(self):\n",
        "        MockDAG._current_dag = self\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        MockDAG._current_dag = None\n",
        "        logger.info(f\"✅ DAG '{self.dag_id}' configurado con {len(self.tasks)} tareas\")\n",
        "\n",
        "    def add_task(self, task):\n",
        "        \"\"\"Agregar tarea al DAG\"\"\"\n",
        "        self.tasks.append(task)\n",
        "        self.task_dict[task.task_id] = task\n",
        "        task.dag = self\n",
        "\n",
        "    def run_all_tasks(self):\n",
        "        \"\"\"Ejecutar todas las tareas del DAG secuencialmente\"\"\"\n",
        "        logger.info(f\"🚀 Iniciando ejecución del DAG: {self.dag_id}\")\n",
        "        results = {}\n",
        "\n",
        "        for task in self.tasks:\n",
        "            try:\n",
        "                logger.info(f\"▶️  Ejecutando tarea: {task.task_id}\")\n",
        "                result = task.execute()\n",
        "                results[task.task_id] = result\n",
        "                logger.info(f\"✅ Tarea {task.task_id} completada\")\n",
        "                if result:\n",
        "                    print(f\"   📊 Resultado: {result}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"❌ Error en tarea {task.task_id}: {str(e)}\")\n",
        "                results[task.task_id] = f\"ERROR: {str(e)}\"\n",
        "\n",
        "        logger.info(f\"🎉 DAG {self.dag_id} completado\")\n",
        "        return results\n",
        "\n",
        "\n",
        "class MockPythonOperator:\n",
        "    \"\"\"Simulador de PythonOperator sin dependencias de BD\"\"\"\n",
        "\n",
        "    def __init__(self, task_id, python_callable=None, op_args=None,\n",
        "                 op_kwargs=None, dag=None, **kwargs):\n",
        "        self.task_id = task_id\n",
        "        self.python_callable = python_callable\n",
        "        self.op_args = op_args or []\n",
        "        self.op_kwargs = op_kwargs or {}\n",
        "        self.dag = dag\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "        # Auto-agregar al DAG actual si existe\n",
        "        if dag:\n",
        "            dag.add_task(self)\n",
        "        elif MockDAG._current_dag:\n",
        "            MockDAG._current_dag.add_task(self)\n",
        "\n",
        "        logger.info(f\"🔧 PythonOperator '{task_id}' registrado\")\n",
        "\n",
        "    def execute(self):\n",
        "        \"\"\"Ejecutar la función Python\"\"\"\n",
        "        if self.python_callable:\n",
        "            try:\n",
        "                # Crear contexto mock\n",
        "                context = {\n",
        "                    'dag': self.dag,\n",
        "                    'task': self,\n",
        "                    'task_instance': self,\n",
        "                    'execution_date': datetime.now(),\n",
        "                    'ds': datetime.now().strftime('%Y-%m-%d'),\n",
        "                    'ts': datetime.now().isoformat(),\n",
        "                }\n",
        "\n",
        "                # Ejecutar función con argumentos\n",
        "                if self.op_kwargs:\n",
        "                    return self.python_callable(*self.op_args, **self.op_kwargs, **context)\n",
        "                else:\n",
        "                    return self.python_callable(*self.op_args, **context)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error ejecutando {self.task_id}: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            logger.info(f\"Tarea {self.task_id} sin función asociada\")\n",
        "            return None\n",
        "\n",
        "\n",
        "class MockEmptyOperator:\n",
        "    \"\"\"Simulador de EmptyOperator (antes DummyOperator)\"\"\"\n",
        "\n",
        "    def __init__(self, task_id, dag=None, **kwargs):\n",
        "        self.task_id = task_id\n",
        "        self.dag = dag\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "        # Auto-agregar al DAG actual si existe\n",
        "        if dag:\n",
        "            dag.add_task(self)\n",
        "        elif MockDAG._current_dag:\n",
        "            MockDAG._current_dag.add_task(self)\n",
        "\n",
        "        logger.info(f\"⭕ EmptyOperator '{task_id}' registrado\")\n",
        "\n",
        "    def execute(self):\n",
        "        \"\"\"Simular ejecución vacía\"\"\"\n",
        "        logger.info(f\"⏭️  Ejecutando tarea vacía: {self.task_id}\")\n",
        "        return f\"Tarea {self.task_id} ejecutada exitosamente\"\n",
        "\n",
        "\n",
        "# Funciones de utilidad para testing\n",
        "def ejecutar_tarea_individual(dag, task_id):\n",
        "    \"\"\"Ejecutar una tarea específica del DAG\"\"\"\n",
        "    if task_id in dag.task_dict:\n",
        "        task = dag.task_dict[task_id]\n",
        "        logger.info(f\"🎯 Ejecutando tarea individual: {task_id}\")\n",
        "        return task.execute()\n",
        "    else:\n",
        "        logger.error(f\"❌ Tarea '{task_id}' no encontrada en el DAG\")\n",
        "        return None\n",
        "\n",
        "def listar_tareas_dag(dag):\n",
        "    \"\"\"Mostrar todas las tareas de un DAG\"\"\"\n",
        "    print(f\"\\n📋 Tareas en DAG '{dag.dag_id}':\")\n",
        "    for i, task in enumerate(dag.tasks, 1):\n",
        "        task_type = \"Python\" if hasattr(task, 'python_callable') else \"Empty\"\n",
        "        print(f\"  {i}. {task.task_id} ({task_type})\")\n",
        "    print()\n",
        "\n",
        "def validar_estructura_dag(dag):\n",
        "    \"\"\"Validar que el DAG esté bien estructurado\"\"\"\n",
        "    print(f\"\\n🔍 Validando DAG: {dag.dag_id}\")\n",
        "    print(f\"  ✓ Número de tareas: {len(dag.tasks)}\")\n",
        "    print(f\"  ✓ Schedule: {dag.schedule_interval}\")\n",
        "    print(f\"  ✓ Start date: {dag.start_date}\")\n",
        "    print(f\"  ✓ Tags: {dag.tags}\")\n",
        "\n",
        "    # Validar tareas Python\n",
        "    python_tasks = [t for t in dag.tasks if hasattr(t, 'python_callable')]\n",
        "    empty_tasks = [t for t in dag.tasks if not hasattr(t, 'python_callable')]\n",
        "\n",
        "    print(f\"  ✓ Tareas Python: {len(python_tasks)}\")\n",
        "    print(f\"  ✓ Tareas Empty: {len(empty_tasks)}\")\n",
        "\n",
        "    # Verificar funciones Python\n",
        "    missing_functions = [t.task_id for t in python_tasks if t.python_callable is None]\n",
        "    if missing_functions:\n",
        "        print(f\"  ⚠️  Tareas sin función: {missing_functions}\")\n",
        "    else:\n",
        "        print(\"  ✅ Todas las tareas Python tienen funciones asignadas\")\n",
        "\n",
        "    print(\"✅ Validación completada\\n\")\n",
        "\n",
        "# Crear aliases para compatibilidad con código Airflow existente\n",
        "DAG = MockDAG\n",
        "PythonOperator = MockPythonOperator\n",
        "EmptyOperator = MockEmptyOperator\n",
        "DummyOperator = MockEmptyOperator  # Alias para compatibilidad\n",
        "\n",
        "# Variable global para DAG actual\n",
        "MockDAG._current_dag = None\n",
        "\n",
        "print(\"🎉 ¡Airflow Mock cargado exitosamente!\")\n",
        "print(\"✅ Puedes usar DAG, PythonOperator y EmptyOperator normalmente\")\n",
        "print(\"🧪 Funciones adicionales:\")\n",
        "print(\"   - dag.run_all_tasks() - Ejecutar todo el DAG\")\n",
        "print(\"   - ejecutar_tarea_individual(dag, 'task_id') - Ejecutar una tarea\")\n",
        "print(\"   - listar_tareas_dag(dag) - Ver todas las tareas\")\n",
        "print(\"   - validar_estructura_dag(dag) - Validar el DAG\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2vuI7pfiKj-",
        "outputId": "9bf858ca-6a97-4255-9f40-316c4694f3c5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎉 ¡Airflow Mock cargado exitosamente!\n",
            "✅ Puedes usar DAG, PythonOperator y EmptyOperator normalmente\n",
            "🧪 Funciones adicionales:\n",
            "   - dag.run_all_tasks() - Ejecutar todo el DAG\n",
            "   - ejecutar_tarea_individual(dag, 'task_id') - Ejecutar una tarea\n",
            "   - listar_tareas_dag(dag) - Ver todas las tareas\n",
            "   - validar_estructura_dag(dag) - Validar el DAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SRI_Vehiculos_ETL_DAG.py\n",
        "# Implementación completa del proceso ETL para datos vehiculares del SRI\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.operators.empty import EmptyOperator\n",
        "import pandas as pd\n",
        "from google.cloud import storage, bigquery\n",
        "import hashlib\n",
        "from io import StringIO\n",
        "import logging\n",
        "DummyOperator = EmptyOperator\n",
        "\n",
        "\n",
        "# Configuración por defecto del DAG\n",
        "default_args = {\n",
        "    'owner': 'sri_data_engineer',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 2,\n",
        "    'retry_delay': timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "# Definición del DAG\n",
        "dag = DAG(\n",
        "    'sri_vehiculos_etl_proceso',\n",
        "    default_args=default_args,\n",
        "    description='Proceso ETL completo para datos vehiculares del SRI',\n",
        "    schedule_interval='@daily',\n",
        "    catchup=False,\n",
        "    tags=['sri', 'vehiculos', 'etl', 'bigquery'],\n",
        "    max_active_runs=1\n",
        ")\n",
        "\n",
        "# Variables de configuración\n",
        "PROJECT_ID = 'sri-vehiculos-etl'  # Reemplazar con tu project ID\n",
        "DATASET_ID = 'sri_vehiculos_dw'\n",
        "BUCKET_NAME = 'sri-vehiculos-etl-bucket-angel'  # Reemplazar con tu bucket\n",
        "\n",
        "# ===============================\n",
        "# FUNCIONES ETL PARA DIMENSIONES\n",
        "# ===============================\n",
        "\n",
        "def etl_dim_tiempo(**context):\n",
        "    \"\"\"\n",
        "    Proceso ETL para la dimensión Tiempo\n",
        "    Genera un rango completo de fechas desde 2020 hasta 2025\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"🕐 Iniciando ETL para Dim_Tiempo...\")\n",
        "\n",
        "        # Configurar cliente de BigQuery\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Generar rango de fechas\n",
        "        start_date = datetime(2020, 1, 1)\n",
        "        end_date = datetime(2025, 12, 31)\n",
        "        fechas = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "        logging.info(f\"📅 Generando {len(fechas)} registros de fechas...\")\n",
        "\n",
        "        # Crear DataFrame de dimensión tiempo\n",
        "        dim_tiempo = pd.DataFrame({\n",
        "            'ID_Tiempo': range(1, len(fechas) + 1),\n",
        "            'FechaCompleta': fechas.date,\n",
        "            'Anio': fechas.year,\n",
        "            'Trimestre': fechas.quarter,\n",
        "            'Mes': fechas.month,\n",
        "            'Dia': fechas.day,\n",
        "            'NombreMes': fechas.strftime('%B'),\n",
        "            'NombreDiaSemana': fechas.strftime('%A')\n",
        "        })\n",
        "\n",
        "        # Traducir nombres al español\n",
        "        meses_es = {\n",
        "            'January': 'Enero', 'February': 'Febrero', 'March': 'Marzo',\n",
        "            'April': 'Abril', 'May': 'Mayo', 'June': 'Junio',\n",
        "            'July': 'Julio', 'August': 'Agosto', 'September': 'Septiembre',\n",
        "            'October': 'Octubre', 'November': 'Noviembre', 'December': 'Diciembre'\n",
        "        }\n",
        "\n",
        "        dias_es = {\n",
        "            'Monday': 'Lunes', 'Tuesday': 'Martes', 'Wednesday': 'Miércoles',\n",
        "            'Thursday': 'Jueves', 'Friday': 'Viernes', 'Saturday': 'Sábado',\n",
        "            'Sunday': 'Domingo'\n",
        "        }\n",
        "\n",
        "        dim_tiempo['NombreMes'] = dim_tiempo['NombreMes'].map(meses_es)\n",
        "        dim_tiempo['NombreDiaSemana'] = dim_tiempo['NombreDiaSemana'].map(dias_es)\n",
        "\n",
        "        # Cargar a BigQuery\n",
        "        table_id = f'{PROJECT_ID}.{DATASET_ID}.dim_tiempo'\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "            schema=[\n",
        "                bigquery.SchemaField(\"ID_Tiempo\", \"INTEGER\"),\n",
        "                bigquery.SchemaField(\"FechaCompleta\", \"DATE\"),\n",
        "                bigquery.SchemaField(\"Anio\", \"INTEGER\"),\n",
        "                bigquery.SchemaField(\"Trimestre\", \"INTEGER\"),\n",
        "                bigquery.SchemaField(\"Mes\", \"INTEGER\"),\n",
        "                bigquery.SchemaField(\"Dia\", \"INTEGER\"),\n",
        "                bigquery.SchemaField(\"NombreMes\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"NombreDiaSemana\", \"STRING\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        job = client.load_table_from_dataframe(dim_tiempo, table_id, job_config=job_config)\n",
        "        job.result()  # Esperar a que termine\n",
        "\n",
        "        logging.info(f\"✅ Cargados {len(dim_tiempo)} registros en dim_tiempo\")\n",
        "        return f\"Dim_Tiempo cargada exitosamente: {len(dim_tiempo)} registros\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Error en ETL Dim_Tiempo: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def etl_dim_vehiculo(**context):\n",
        "    \"\"\"\n",
        "    Proceso ETL para la dimensión Vehículo\n",
        "    Extrae características únicas de vehículos del archivo CSV\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"🚗 Iniciando ETL para Dim_Vehiculo...\")\n",
        "\n",
        "        # Configurar clientes\n",
        "        storage_client = storage.Client()\n",
        "        bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Extraer datos del bucket\n",
        "        bucket = storage_client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob('raw-data/sri_vehiculos.csv')\n",
        "\n",
        "        # Leer CSV desde Cloud Storage\n",
        "        content = blob.download_as_text()\n",
        "        df = pd.read_csv(StringIO(content))\n",
        "\n",
        "        logging.info(f\"📊 Datos extraídos: {len(df)} registros originales\")\n",
        "\n",
        "        # Seleccionar columnas para la dimensión vehículo\n",
        "        columnas_vehiculo = [\n",
        "            'CÓDIGO DE VEHÍCULO', 'MARCA', 'MODELO', 'PAÍS',\n",
        "            'AÑO MODELO', 'CLASE', 'SUB CLASE', 'TIPO',\n",
        "            'CILINDRAJE', 'TIPO COMBUSTIBLE', 'COLOR 1', 'COLOR 2'\n",
        "        ]\n",
        "\n",
        "        # Verificar que las columnas existen\n",
        "        columnas_existentes = [col for col in columnas_vehiculo if col in df.columns]\n",
        "        if len(columnas_existentes) != len(columnas_vehiculo):\n",
        "            logging.warning(f\"Algunas columnas no encontradas. Usando: {columnas_existentes}\")\n",
        "\n",
        "        # Crear dimensión con registros únicos\n",
        "        dim_vehiculo = df[columnas_existentes].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "        # Generar clave subrogada\n",
        "        dim_vehiculo['ID_Vehiculo'] = range(1, len(dim_vehiculo) + 1)\n",
        "\n",
        "        # Limpiar y estandarizar datos\n",
        "        for col in ['MARCA', 'MODELO', 'PAÍS', 'CLASE', 'SUB CLASE', 'TIPO', 'TIPO COMBUSTIBLE']:\n",
        "            if col in dim_vehiculo.columns:\n",
        "                dim_vehiculo[col] = dim_vehiculo[col].astype(str).str.upper().str.strip()\n",
        "\n",
        "        # Manejar valores nulos\n",
        "        if 'COLOR 2' in dim_vehiculo.columns:\n",
        "            dim_vehiculo['COLOR 2'] = dim_vehiculo['COLOR 2'].fillna('N/A')\n",
        "\n",
        "        # Renombrar columnas para BigQuery (sin espacios ni caracteres especiales)\n",
        "        rename_dict = {\n",
        "            'CÓDIGO DE VEHÍCULO': 'CodigoVehiculo',\n",
        "            'MARCA': 'Marca',\n",
        "            'MODELO': 'Modelo',\n",
        "            'PAÍS': 'Pais',\n",
        "            'AÑO MODELO': 'AnioModelo',\n",
        "            'CLASE': 'Clase',\n",
        "            'SUB CLASE': 'SubClase',\n",
        "            'TIPO': 'Tipo',\n",
        "            'CILINDRAJE': 'Cilindraje',\n",
        "            'TIPO COMBUSTIBLE': 'TipoCombustible',\n",
        "            'COLOR 1': 'Color1',\n",
        "            'COLOR 2': 'Color2'\n",
        "        }\n",
        "\n",
        "        # Solo renombrar columnas que existen\n",
        "        rename_dict_filtered = {k: v for k, v in rename_dict.items() if k in dim_vehiculo.columns}\n",
        "        dim_vehiculo = dim_vehiculo.rename(columns=rename_dict_filtered)\n",
        "\n",
        "        # Reordenar columnas (solo las que existen)\n",
        "        columnas_orden = ['ID_Vehiculo'] + [v for k, v in rename_dict_filtered.items()]\n",
        "        dim_vehiculo = dim_vehiculo[columnas_orden]\n",
        "\n",
        "        logging.info(f\"🔧 Transformación completada: {len(dim_vehiculo)} vehículos únicos\")\n",
        "\n",
        "        # Cargar a BigQuery\n",
        "        table_id = f'{PROJECT_ID}.{DATASET_ID}.dim_vehiculo'\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "        job = bigquery_client.load_table_from_dataframe(dim_vehiculo, table_id, job_config=job_config)\n",
        "        job.result()\n",
        "\n",
        "        logging.info(f\"✅ Cargados {len(dim_vehiculo)} registros en dim_vehiculo\")\n",
        "        return f\"Dim_Vehiculo cargada exitosamente: {len(dim_vehiculo)} registros\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Error en ETL Dim_Vehiculo: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def etl_dim_transaccion(**context):\n",
        "    \"\"\"\n",
        "    Proceso ETL para la dimensión Transacción\n",
        "    Crea combinaciones únicas de tipos de transacción y servicio\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"💼 Iniciando ETL para Dim_Transaccion...\")\n",
        "\n",
        "        # Configurar clientes\n",
        "        storage_client = storage.Client()\n",
        "        bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Extraer datos del bucket\n",
        "        bucket = storage_client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob('raw-data/sri_vehiculos.csv')\n",
        "\n",
        "        content = blob.download_as_text()\n",
        "        df = pd.read_csv(StringIO(content))\n",
        "\n",
        "        # Seleccionar columnas para dimensión transacción\n",
        "        columnas_transaccion = [\n",
        "            'TIPO TRANSACCIÓN', 'TIPO SERVICIO',\n",
        "            'PERSONA NATURAL - JURÍDICA', 'CATEGORÍA'\n",
        "        ]\n",
        "\n",
        "        # Verificar columnas existentes\n",
        "        columnas_existentes = [col for col in columnas_transaccion if col in df.columns]\n",
        "        logging.info(f\"Columnas encontradas: {columnas_existentes}\")\n",
        "\n",
        "        # Crear dimensión con combinaciones únicas\n",
        "        dim_transaccion = df[columnas_existentes].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "        # Generar clave subrogada\n",
        "        dim_transaccion['ID_Transaccion'] = range(1, len(dim_transaccion) + 1)\n",
        "\n",
        "        # Limpiar datos\n",
        "        for col in columnas_existentes:\n",
        "            if col in dim_transaccion.columns:\n",
        "                dim_transaccion[col] = dim_transaccion[col].astype(str).str.upper().str.strip()\n",
        "\n",
        "        # Renombrar columnas\n",
        "        rename_dict = {\n",
        "            'TIPO TRANSACCIÓN': 'TipoTransaccion',\n",
        "            'TIPO SERVICIO': 'TipoServicio',\n",
        "            'PERSONA NATURAL - JURÍDICA': 'PersonaTipo',\n",
        "            'CATEGORÍA': 'Categoria'\n",
        "        }\n",
        "\n",
        "        rename_dict_filtered = {k: v for k, v in rename_dict.items() if k in dim_transaccion.columns}\n",
        "        dim_transaccion = dim_transaccion.rename(columns=rename_dict_filtered)\n",
        "\n",
        "        # Reordenar columnas\n",
        "        columnas_orden = ['ID_Transaccion'] + [v for k, v in rename_dict_filtered.items()]\n",
        "        dim_transaccion = dim_transaccion[columnas_orden]\n",
        "\n",
        "        logging.info(f\"🔧 Transformación completada: {len(dim_transaccion)} tipos de transacción únicos\")\n",
        "\n",
        "        # Cargar a BigQuery\n",
        "        table_id = f'{PROJECT_ID}.{DATASET_ID}.dim_transaccion'\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "        job = bigquery_client.load_table_from_dataframe(dim_transaccion, table_id, job_config=job_config)\n",
        "        job.result()\n",
        "\n",
        "        logging.info(f\"✅ Cargados {len(dim_transaccion)} registros en dim_transaccion\")\n",
        "        return f\"Dim_Transaccion cargada exitosamente: {len(dim_transaccion)} registros\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Error en ETL Dim_Transaccion: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def etl_dim_ubicacion(**context):\n",
        "    \"\"\"\n",
        "    Proceso ETL para la dimensión Ubicación\n",
        "    Mapea códigos de cantón a información geográfica completa\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"🌎 Iniciando ETL para Dim_Ubicacion...\")\n",
        "\n",
        "        # Configurar clientes\n",
        "        storage_client = storage.Client()\n",
        "        bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Extraer datos del bucket\n",
        "        bucket = storage_client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob('raw-data/sri_vehiculos.csv')\n",
        "\n",
        "        content = blob.download_as_text()\n",
        "        df = pd.read_csv(StringIO(content))\n",
        "\n",
        "        # Mapeo de cantones expandido\n",
        "        mapeo_cantones = {\n",
        "            '10701': {'canton': 'CUENCA', 'provincia': 'AZUAY', 'region': 'SIERRA'},\n",
        "            '10911': {'canton': 'GIRON', 'provincia': 'AZUAY', 'region': 'SIERRA'},\n",
        "            '10901': {'canton': 'GUALACEO', 'provincia': 'AZUAY', 'region': 'SIERRA'},\n",
        "            '10927': {'canton': 'SANTA ISABEL', 'provincia': 'AZUAY', 'region': 'SIERRA'},\n",
        "            '20606': {'canton': 'PLAYAS', 'provincia': 'GUAYAS', 'region': 'COSTA'},\n",
        "            '21101': {'canton': 'GUAYAQUIL', 'provincia': 'GUAYAS', 'region': 'COSTA'},\n",
        "            '21709': {'canton': 'MILAGRO', 'provincia': 'GUAYAS', 'region': 'COSTA'},\n",
        "            '31905': {'canton': 'ZAMORA', 'provincia': 'ZAMORA CHINCHIPE', 'region': 'AMAZONIA'},\n",
        "            '20501': {'canton': 'QUITO', 'provincia': 'PICHINCHA', 'region': 'SIERRA'},\n",
        "            '20505': {'canton': 'CAYAMBE', 'provincia': 'PICHINCHA', 'region': 'SIERRA'},\n",
        "            '30101': {'canton': 'LAGO AGRIO', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30201': {'canton': 'GONZALO PIZARRO', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30301': {'canton': 'PUTUMAYO', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30401': {'canton': 'SHUSHUFINDI', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30501': {'canton': 'SUCUMBIOS', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30601': {'canton': 'CASCALES', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30701': {'canton': 'CUYABENO', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "        }\n",
        "\n",
        "        # Verificar si la columna CANTON existe\n",
        "        col_canton = None\n",
        "        for col in ['CANTON', 'CANTÓN', 'canton', 'cantón']:\n",
        "            if col in df.columns:\n",
        "                col_canton = col\n",
        "                break\n",
        "\n",
        "        if col_canton is None:\n",
        "            logging.warning(\"No se encontró columna de cantón. Usando ubicación genérica.\")\n",
        "            # Crear una ubicación por defecto\n",
        "            dim_ubicacion = pd.DataFrame([{\n",
        "                'ID_Ubicacion': 1,\n",
        "                'CodigoCanton': '99999',\n",
        "                'NombreCanton': 'NO_ESPECIFICADO',\n",
        "                'Provincia': 'NO_ESPECIFICADA',\n",
        "                'Region': 'NO_ESPECIFICADA',\n",
        "                'Pais': 'ECUADOR'\n",
        "            }])\n",
        "        else:\n",
        "            # Obtener cantones únicos del dataset\n",
        "            cantones_dataset = df[col_canton].dropna().unique()\n",
        "\n",
        "            # Crear dimensión ubicación\n",
        "            ubicaciones = []\n",
        "            id_counter = 1\n",
        "\n",
        "            for codigo_canton in cantones_dataset:\n",
        "                codigo_str = str(codigo_canton).strip()\n",
        "                if codigo_str in mapeo_cantones:\n",
        "                    info = mapeo_cantones[codigo_str]\n",
        "                    ubicaciones.append({\n",
        "                        'ID_Ubicacion': id_counter,\n",
        "                        'CodigoCanton': codigo_str,\n",
        "                        'NombreCanton': info['canton'],\n",
        "                        'Provincia': info['provincia'],\n",
        "                        'Region': info['region'],\n",
        "                        'Pais': 'ECUADOR'\n",
        "                    })\n",
        "                else:\n",
        "                    # Para cantones no mapeados, crear entrada genérica\n",
        "                    ubicaciones.append({\n",
        "                        'ID_Ubicacion': id_counter,\n",
        "                        'CodigoCanton': codigo_str,\n",
        "                        'NombreCanton': f'CANTON_{codigo_str}',\n",
        "                        'Provincia': 'NO_IDENTIFICADA',\n",
        "                        'Region': 'NO_IDENTIFICADA',\n",
        "                        'Pais': 'ECUADOR'\n",
        "                    })\n",
        "                id_counter += 1\n",
        "\n",
        "            dim_ubicacion = pd.DataFrame(ubicaciones)\n",
        "\n",
        "        logging.info(f\"🔧 Transformación completada: {len(dim_ubicacion)} ubicaciones únicas\")\n",
        "\n",
        "        # Cargar a BigQuery\n",
        "        table_id = f'{PROJECT_ID}.{DATASET_ID}.dim_ubicacion'\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "        job = bigquery_client.load_table_from_dataframe(dim_ubicacion, table_id, job_config=job_config)\n",
        "        job.result()\n",
        "\n",
        "        logging.info(f\"✅ Cargados {len(dim_ubicacion)} registros en dim_ubicacion\")\n",
        "        return f\"Dim_Ubicacion cargada exitosamente: {len(dim_ubicacion)} registros\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Error en ETL Dim_Ubicacion: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ===============================\n",
        "# FUNCIÓN ETL PARA TABLA DE HECHOS\n",
        "# ===============================\n",
        "\n",
        "def etl_fact_registro_vehiculos(**context):\n",
        "    \"\"\"\n",
        "    Proceso ETL para la tabla de hechos Fact_RegistroVehiculos\n",
        "    Realiza lookups con las dimensiones y carga métricas\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"📊 Iniciando ETL para Fact_RegistroVehiculos...\")\n",
        "\n",
        "        # Configurar clientes\n",
        "        storage_client = storage.Client()\n",
        "        bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Extraer datos principales del bucket\n",
        "        bucket = storage_client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob('raw-data/sri_vehiculos.csv')\n",
        "\n",
        "        content = blob.download_as_text()\n",
        "        df_hechos = pd.read_csv(StringIO(content))\n",
        "\n",
        "        logging.info(f\"📊 Datos extraídos: {len(df_hechos)} registros de hechos\")\n",
        "\n",
        "        # Cargar dimensiones desde BigQuery para lookups\n",
        "        logging.info(\"🔍 Cargando dimensiones para lookups...\")\n",
        "\n",
        "        try:\n",
        "            # Cargar Dim_Tiempo\n",
        "            query_tiempo = f\"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.dim_tiempo`\"\n",
        "            dim_tiempo = bigquery_client.query(query_tiempo).to_dataframe()\n",
        "\n",
        "            # Cargar Dim_Vehiculo\n",
        "            query_vehiculo = f\"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.dim_vehiculo`\"\n",
        "            dim_vehiculo = bigquery_client.query(query_vehiculo).to_dataframe()\n",
        "\n",
        "            # Cargar Dim_Transaccion\n",
        "            query_transaccion = f\"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.dim_transaccion`\"\n",
        "            dim_transaccion = bigquery_client.query(query_transaccion).to_dataframe()\n",
        "\n",
        "            # Cargar Dim_Ubicacion\n",
        "            query_ubicacion = f\"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.dim_ubicacion`\"\n",
        "            dim_ubicacion = bigquery_client.query(query_ubicacion).to_dataframe()\n",
        "\n",
        "            logging.info(\"✅ Dimensiones cargadas para lookups\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error cargando dimensiones: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # Procesamiento de fechas\n",
        "        logging.info(\"📅 Procesando fechas...\")\n",
        "\n",
        "        # Buscar columna de fecha\n",
        "        col_fecha = None\n",
        "        for col in ['FECHA PROCESO', 'FECHA_PROCESO', 'fecha_proceso', 'FECHA']:\n",
        "            if col in df_hechos.columns:\n",
        "                col_fecha = col\n",
        "                break\n",
        "\n",
        "        if col_fecha:\n",
        "            try:\n",
        "                df_hechos['FECHA_PROCESO_CONV'] = pd.to_datetime(df_hechos[col_fecha], errors='coerce')\n",
        "                # Filtrar fechas válidas\n",
        "                df_hechos = df_hechos.dropna(subset=['FECHA_PROCESO_CONV'])\n",
        "                df_hechos['FECHA_PROCESO_DATE'] = df_hechos['FECHA_PROCESO_CONV'].dt.date\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Error procesando fechas: {str(e)}. Usando fecha por defecto.\")\n",
        "                df_hechos['FECHA_PROCESO_DATE'] = datetime.now().date()\n",
        "        else:\n",
        "            logging.warning(\"No se encontró columna de fecha. Usando fecha actual.\")\n",
        "            df_hechos['FECHA_PROCESO_DATE'] = datetime.now().date()\n",
        "\n",
        "        # Realizar lookups con dimensiones\n",
        "        logging.info(\"🔗 Realizando lookups con dimensiones...\")\n",
        "\n",
        "        # Lookup con Dim_Tiempo\n",
        "        df_hechos = df_hechos.merge(\n",
        "            dim_tiempo[['ID_Tiempo', 'FechaCompleta']],\n",
        "            left_on='FECHA_PROCESO_DATE',\n",
        "            right_on='FechaCompleta',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Lookup con Dim_Vehiculo (usando código de vehículo)\n",
        "        col_codigo_vehiculo = None\n",
        "        for col in ['CÓDIGO DE VEHÍCULO', 'CODIGO_VEHICULO', 'codigo_vehiculo']:\n",
        "            if col in df_hechos.columns:\n",
        "                col_codigo_vehiculo = col\n",
        "                break\n",
        "\n",
        "        if col_codigo_vehiculo:\n",
        "            df_hechos = df_hechos.merge(\n",
        "                dim_vehiculo[['ID_Vehiculo', 'CodigoVehiculo']],\n",
        "                left_on=col_codigo_vehiculo,\n",
        "                right_on='CodigoVehiculo',\n",
        "                how='left'\n",
        "            )\n",
        "        else:\n",
        "            df_hechos['ID_Vehiculo'] = 1  # ID por defecto\n",
        "\n",
        "        # Lookup con Dim_Transaccion\n",
        "        merge_cols = []\n",
        "        if 'TIPO TRANSACCIÓN' in df_hechos.columns and 'TipoTransaccion' in dim_transaccion.columns:\n",
        "            merge_cols.append(('TIPO TRANSACCIÓN', 'TipoTransaccion'))\n",
        "        if 'TIPO SERVICIO' in df_hechos.columns and 'TipoServicio' in dim_transaccion.columns:\n",
        "            merge_cols.append(('TIPO SERVICIO', 'TipoServicio'))\n",
        "\n",
        "        if merge_cols:\n",
        "            left_cols = [col[0] for col in merge_cols]\n",
        "            right_cols = [col[1] for col in merge_cols]\n",
        "            df_hechos = df_hechos.merge(\n",
        "                dim_transaccion[['ID_Transaccion'] + right_cols],\n",
        "                left_on=left_cols,\n",
        "                right_on=right_cols,\n",
        "                how='left'\n",
        "            )\n",
        "        else:\n",
        "            df_hechos['ID_Transaccion'] = 1  # ID por defecto\n",
        "\n",
        "        # Lookup con Dim_Ubicacion\n",
        "        col_canton = None\n",
        "        for col in ['CANTON', 'CANTÓN', 'canton']:\n",
        "            if col in df_hechos.columns:\n",
        "                col_canton = col\n",
        "                break\n",
        "\n",
        "        if col_canton:\n",
        "            df_hechos[col_canton] = df_hechos[col_canton].astype(str)\n",
        "            df_hechos = df_hechos.merge(\n",
        "                dim_ubicacion[['ID_Ubicacion', 'CodigoCanton']],\n",
        "                left_on=col_canton,\n",
        "                right_on='CodigoCanton',\n",
        "                how='left'\n",
        "            )\n",
        "        else:\n",
        "            df_hechos['ID_Ubicacion'] = 1  # ID por defecto\n",
        "\n",
        "        # Crear tabla de hechos final\n",
        "        logging.info(\"📋 Creando tabla de hechos final...\")\n",
        "\n",
        "        # Generar ID único para cada registro\n",
        "        df_hechos['ID_Registro'] = range(1, len(df_hechos) + 1)\n",
        "\n",
        "        # Calcular métricas\n",
        "        df_hechos['CantidadRegistros'] = 1\n",
        "\n",
        "        # Buscar columna de avalúo\n",
        "        col_avaluo = None\n",
        "        for col in ['AVALUO', 'AVALÚO', 'avaluo', 'avalúo']:\n",
        "            if col in df_hechos.columns:\n",
        "                col_avaluo = col\n",
        "                break\n",
        "\n",
        "        if col_avaluo:\n",
        "            df_hechos['MontoAvaluo'] = pd.to_numeric(df_hechos[col_avaluo], errors='coerce').fillna(0)\n",
        "        else:\n",
        "            df_hechos['MontoAvaluo'] = 0\n",
        "\n",
        "        # Seleccionar columnas finales para la tabla de hechos\n",
        "        columnas_fact = [\n",
        "            'ID_Registro',\n",
        "            'ID_Tiempo',\n",
        "            'ID_Vehiculo',\n",
        "            'ID_Transaccion',\n",
        "            'ID_Ubicacion',\n",
        "            'CantidadRegistros',\n",
        "            'MontoAvaluo'\n",
        "        ]\n",
        "\n",
        "        # Verificar que todas las columnas existen\n",
        "        columnas_existentes = [col for col in columnas_fact if col in df_hechos.columns]\n",
        "        fact_table = df_hechos[columnas_existentes].copy()\n",
        "\n",
        "        # Llenar valores nulos con defaults\n",
        "        for col in ['ID_Tiempo', 'ID_Vehiculo', 'ID_Transaccion', 'ID_Ubicacion']:\n",
        "            if col in fact_table.columns:\n",
        "                fact_table[col] = fact_table[col].fillna(1)\n",
        "\n",
        "        fact_table = fact_table.fillna(0)\n",
        "\n",
        "        logging.info(f\"🔧 Tabla de hechos creada: {len(fact_table)} registros\")\n",
        "\n",
        "        # Cargar a BigQuery\n",
        "        table_id = f'{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos'\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "        job = bigquery_client.load_table_from_dataframe(fact_table, table_id, job_config=job_config)\n",
        "        job.result()\n",
        "\n",
        "        logging.info(f\"✅ Cargados {len(fact_table)} registros en fact_registro_vehiculos\")\n",
        "        return f\"Fact_RegistroVehiculos cargada exitosamente: {len(fact_table)} registros\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Error en ETL Fact_RegistroVehiculos: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ===============================\n",
        "# DEFINICIÓN DE TAREAS DEL DAG\n",
        "# ===============================\n",
        "\n",
        "# Tarea de inicio\n",
        "inicio = DummyOperator(\n",
        "    task_id='inicio_proceso_etl',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Tareas ETL para dimensiones\n",
        "tarea_dim_tiempo = PythonOperator(\n",
        "    task_id='etl_dim_tiempo',\n",
        "    python_callable=etl_dim_tiempo,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "tarea_dim_vehiculo = PythonOperator(\n",
        "    task_id='etl_dim_vehiculo',\n",
        "    python_callable=etl_dim_vehiculo,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "tarea_dim_transaccion = PythonOperator(\n",
        "    task_id='etl_dim_transaccion',\n",
        "    python_callable=etl_dim_transaccion,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "tarea_dim_ubicacion = PythonOperator(\n",
        "    task_id='etl_dim_ubicacion',\n",
        "    python_callable=etl_dim_ubicacion,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Tarea de sincronización para dimensiones\n",
        "sincronizacion_dimensiones = DummyOperator(\n",
        "    task_id='sincronizacion_dimensiones',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Tarea ETL para tabla de hechos\n",
        "tarea_fact_registro = PythonOperator(\n",
        "    task_id='etl_fact_registro_vehiculos',\n",
        "    python_callable=etl_fact_registro_vehiculos,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Tarea de finalización\n",
        "finalizacion = DummyOperator(\n",
        "    task_id='finalizacion_proceso_etl',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# FUNCIONES DE VALIDACIÓN Y MONITOREO\n",
        "# ===============================\n",
        "\n",
        "def validar_calidad_datos(**context):\n",
        "    \"\"\"\n",
        "    Función para validar la calidad de los datos cargados\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"🔍 Iniciando validación de calidad de datos...\")\n",
        "\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Validaciones para dimensiones\n",
        "        validaciones = []\n",
        "\n",
        "        # Validar Dim_Tiempo\n",
        "        query_tiempo = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_registros,\n",
        "            COUNT(DISTINCT Anio) as anios_unicos,\n",
        "            MIN(FechaCompleta) as fecha_min,\n",
        "            MAX(FechaCompleta) as fecha_max\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.dim_tiempo`\n",
        "        \"\"\"\n",
        "\n",
        "        result_tiempo = client.query(query_tiempo).to_dataframe()\n",
        "        validaciones.append(f\"Dim_Tiempo: {result_tiempo.iloc[0]['total_registros']} registros, \"\n",
        "                          f\"años {result_tiempo.iloc[0]['anios_unicos']}, \"\n",
        "                          f\"rango: {result_tiempo.iloc[0]['fecha_min']} a {result_tiempo.iloc[0]['fecha_max']}\")\n",
        "\n",
        "        # Validar Dim_Vehiculo\n",
        "        query_vehiculo = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_registros,\n",
        "            COUNT(DISTINCT Marca) as marcas_unicas,\n",
        "            COUNT(DISTINCT Clase) as clases_unicas\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.dim_vehiculo`\n",
        "        \"\"\"\n",
        "\n",
        "        result_vehiculo = client.query(query_vehiculo).to_dataframe()\n",
        "        validaciones.append(f\"Dim_Vehiculo: {result_vehiculo.iloc[0]['total_registros']} registros, \"\n",
        "                          f\"{result_vehiculo.iloc[0]['marcas_unicas']} marcas, \"\n",
        "                          f\"{result_vehiculo.iloc[0]['clases_unicas']} clases\")\n",
        "\n",
        "        # Validar Dim_Transaccion\n",
        "        query_transaccion = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_registros,\n",
        "            COUNT(DISTINCT TipoTransaccion) as tipos_transaccion\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.dim_transaccion`\n",
        "        \"\"\"\n",
        "\n",
        "        result_transaccion = client.query(query_transaccion).to_dataframe()\n",
        "        validaciones.append(f\"Dim_Transaccion: {result_transaccion.iloc[0]['total_registros']} registros, \"\n",
        "                          f\"{result_transaccion.iloc[0]['tipos_transaccion']} tipos de transacción\")\n",
        "\n",
        "        # Validar Dim_Ubicacion\n",
        "        query_ubicacion = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_registros,\n",
        "            COUNT(DISTINCT Provincia) as provincias_unicas,\n",
        "            COUNT(DISTINCT Region) as regiones_unicas\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.dim_ubicacion`\n",
        "        \"\"\"\n",
        "\n",
        "        result_ubicacion = client.query(query_ubicacion).to_dataframe()\n",
        "        validaciones.append(f\"Dim_Ubicacion: {result_ubicacion.iloc[0]['total_registros']} registros, \"\n",
        "                          f\"{result_ubicacion.iloc[0]['provincias_unicas']} provincias, \"\n",
        "                          f\"{result_ubicacion.iloc[0]['regiones_unicas']} regiones\")\n",
        "\n",
        "        # Validar Fact_RegistroVehiculos\n",
        "        query_fact = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_registros,\n",
        "            SUM(CantidadRegistros) as total_cantidad,\n",
        "            AVG(MontoAvaluo) as avaluo_promedio,\n",
        "            COUNT(CASE WHEN ID_Tiempo IS NULL THEN 1 END) as registros_sin_tiempo,\n",
        "            COUNT(CASE WHEN ID_Vehiculo IS NULL THEN 1 END) as registros_sin_vehiculo\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos`\n",
        "        \"\"\"\n",
        "\n",
        "        result_fact = client.query(query_fact).to_dataframe()\n",
        "        validaciones.append(f\"Fact_RegistroVehiculos: {result_fact.iloc[0]['total_registros']} registros, \"\n",
        "                          f\"cantidad total: {result_fact.iloc[0]['total_cantidad']}, \"\n",
        "                          f\"avalúo promedio: ${result_fact.iloc[0]['avaluo_promedio']:,.2f}\")\n",
        "\n",
        "        # Log de todas las validaciones\n",
        "        for validacion in validaciones:\n",
        "            logging.info(f\"✅ {validacion}\")\n",
        "\n",
        "        # Verificar integridad referencial\n",
        "        query_integridad = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as registros_con_claves_validas\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos` f\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_tiempo` t ON f.ID_Tiempo = t.ID_Tiempo\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_vehiculo` v ON f.ID_Vehiculo = v.ID_Vehiculo\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_transaccion` tr ON f.ID_Transaccion = tr.ID_Transaccion\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_ubicacion` u ON f.ID_Ubicacion = u.ID_Ubicacion\n",
        "        \"\"\"\n",
        "\n",
        "        result_integridad = client.query(query_integridad).to_dataframe()\n",
        "        registros_validos = result_integridad.iloc[0]['registros_con_claves_validas']\n",
        "\n",
        "        logging.info(f\"🔗 Integridad referencial: {registros_validos} registros con todas las claves válidas\")\n",
        "\n",
        "        resumen_validacion = {\n",
        "            'validaciones': validaciones,\n",
        "            'registros_con_integridad': registros_validos,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return resumen_validacion\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Error en validación de calidad: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def generar_metricas_negocio(**context):\n",
        "    \"\"\"\n",
        "    Genera métricas de negocio del proceso ETL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"📈 Generando métricas de negocio...\")\n",
        "\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Métricas por año\n",
        "        query_por_anio = f\"\"\"\n",
        "        SELECT\n",
        "            t.Anio,\n",
        "            COUNT(*) as total_registros,\n",
        "            SUM(f.MontoAvaluo) as monto_total_avaluo,\n",
        "            AVG(f.MontoAvaluo) as monto_promedio_avaluo\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos` f\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_tiempo` t ON f.ID_Tiempo = t.ID_Tiempo\n",
        "        GROUP BY t.Anio\n",
        "        ORDER BY t.Anio DESC\n",
        "        LIMIT 5\n",
        "        \"\"\"\n",
        "\n",
        "        metricas_anio = client.query(query_por_anio).to_dataframe()\n",
        "\n",
        "        # Métricas por marca\n",
        "        query_por_marca = f\"\"\"\n",
        "        SELECT\n",
        "            v.Marca,\n",
        "            COUNT(*) as total_registros,\n",
        "            AVG(f.MontoAvaluo) as avaluo_promedio\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos` f\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_vehiculo` v ON f.ID_Vehiculo = v.ID_Vehiculo\n",
        "        GROUP BY v.Marca\n",
        "        ORDER BY total_registros DESC\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "\n",
        "        metricas_marca = client.query(query_por_marca).to_dataframe()\n",
        "\n",
        "        # Métricas por provincia\n",
        "        query_por_provincia = f\"\"\"\n",
        "        SELECT\n",
        "            u.Provincia,\n",
        "            u.Region,\n",
        "            COUNT(*) as total_registros,\n",
        "            SUM(f.MontoAvaluo) as monto_total\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos` f\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_ubicacion` u ON f.ID_Ubicacion = u.ID_Ubicacion\n",
        "        GROUP BY u.Provincia, u.Region\n",
        "        ORDER BY total_registros DESC\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "\n",
        "        metricas_provincia = client.query(query_por_provincia).to_dataframe()\n",
        "\n",
        "        # Log de métricas\n",
        "        logging.info(\"📊 MÉTRICAS POR AÑO:\")\n",
        "        for _, row in metricas_anio.iterrows():\n",
        "            logging.info(f\"   {row['Anio']}: {row['total_registros']} registros, \"\n",
        "                        f\"avalúo total: ${row['monto_total_avaluo']:,.2f}\")\n",
        "\n",
        "        logging.info(\"🚗 TOP MARCAS:\")\n",
        "        for _, row in metricas_marca.iterrows():\n",
        "            logging.info(f\"   {row['Marca']}: {row['total_registros']} registros, \"\n",
        "                        f\"avalúo promedio: ${row['avaluo_promedio']:,.2f}\")\n",
        "\n",
        "        logging.info(\"🌎 TOP PROVINCIAS:\")\n",
        "        for _, row in metricas_provincia.iterrows():\n",
        "            logging.info(f\"   {row['Provincia']} ({row['Region']}): {row['total_registros']} registros\")\n",
        "\n",
        "        metricas_resumen = {\n",
        "            'metricas_por_anio': metricas_anio.to_dict('records'),\n",
        "            'metricas_por_marca': metricas_marca.to_dict('records'),\n",
        "            'metricas_por_provincia': metricas_provincia.to_dict('records'),\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return metricas_resumen\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Error generando métricas: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def notificar_finalizacion(**context):\n",
        "    \"\"\"\n",
        "    Notifica la finalización exitosa del proceso ETL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"📧 Enviando notificación de finalización...\")\n",
        "\n",
        "        # Obtener información del contexto\n",
        "        dag_run = context['dag_run']\n",
        "        execution_date = context['execution_date']\n",
        "\n",
        "        # Crear resumen del proceso\n",
        "        resumen = {\n",
        "            'dag_id': dag_run.dag_id,\n",
        "            'execution_date': execution_date.isoformat(),\n",
        "            'estado': 'EXITOSO',\n",
        "            'duracion_total': str(datetime.now() - dag_run.start_date) if dag_run.start_date else 'N/A',\n",
        "            'timestamp_finalizacion': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        logging.info(\"✅ PROCESO ETL FINALIZADO EXITOSAMENTE\")\n",
        "        logging.info(f\"   DAG: {resumen['dag_id']}\")\n",
        "        logging.info(f\"   Fecha de ejecución: {resumen['execution_date']}\")\n",
        "        logging.info(f\"   Duración: {resumen['duracion_total']}\")\n",
        "        logging.info(f\"   Estado: {resumen['estado']}\")\n",
        "\n",
        "        # Aquí se puede agregar lógica para enviar emails, Slack, etc.\n",
        "        # Por ejemplo:\n",
        "        # send_email_notification(resumen)\n",
        "        # send_slack_notification(resumen)\n",
        "\n",
        "        return resumen\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"❌ Error en notificación: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ===============================\n",
        "# TAREAS DE VALIDACIÓN Y MONITOREO\n",
        "# ===============================\n",
        "\n",
        "tarea_validacion = PythonOperator(\n",
        "    task_id='validar_calidad_datos',\n",
        "    python_callable=validar_calidad_datos,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "tarea_metricas = PythonOperator(\n",
        "    task_id='generar_metricas_negocio',\n",
        "    python_callable=generar_metricas_negocio,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "tarea_notificacion = PythonOperator(\n",
        "    task_id='notificar_finalizacion',\n",
        "    python_callable=notificar_finalizacion,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# DEFINICIÓN DE DEPENDENCIAS DEL DAG\n",
        "# ===============================\n",
        "\n",
        "# Estructura de dependencias:\n",
        "# inicio -> [dimensiones en paralelo] -> sincronización -> tabla_hechos -> validación -> métricas -> notificación -> fin\n",
        "\n",
        "# Inicio del proceso\n",
        "inicio >> [tarea_dim_tiempo, tarea_dim_vehiculo, tarea_dim_transaccion, tarea_dim_ubicacion]\n",
        "\n",
        "# Sincronización de dimensiones\n",
        "[tarea_dim_tiempo, tarea_dim_vehiculo, tarea_dim_transaccion, tarea_dim_ubicacion] >> sincronizacion_dimensiones\n",
        "\n",
        "# Tabla de hechos (después de que todas las dimensiones estén listas)\n",
        "sincronizacion_dimensiones >> tarea_fact_registro\n",
        "\n",
        "# Validación y métricas\n",
        "tarea_fact_registro >> tarea_validacion >> tarea_metricas >> tarea_notificacion >> finalizacion\n",
        "\n",
        "# ===============================\n",
        "# CONFIGURACIÓN ADICIONAL DEL DAG\n",
        "# ===============================\n",
        "\n",
        "# Configurar el DAG para logging detallado\n",
        "dag.doc_md = \"\"\"\n",
        "# DAG ETL SRI Vehículos\n",
        "\n",
        "Este DAG implementa un proceso ETL completo para los datos vehiculares del SRI.\n",
        "\n",
        "## Estructura del Proceso:\n",
        "\n",
        "1. **Dimensiones (Paralelo)**:\n",
        "   - `dim_tiempo`: Genera calendario completo 2020-2025\n",
        "   - `dim_vehiculo`: Extrae características únicas de vehículos\n",
        "   - `dim_transaccion`: Mapea tipos de transacciones\n",
        "   - `dim_ubicacion`: Mapea códigos de cantón a geografía\n",
        "\n",
        "2. **Tabla de Hechos**:\n",
        "   - `fact_registro_vehiculos`: Combina todas las dimensiones con métricas\n",
        "\n",
        "3. **Validación y Monitoreo**:\n",
        "   - Validación de calidad de datos\n",
        "   - Generación de métricas de negocio\n",
        "   - Notificaciones de finalización\n",
        "\n",
        "## Configuración Requerida:\n",
        "\n",
        "- PROJECT_ID: ID del proyecto de Google Cloud\n",
        "- DATASET_ID: Nombre del dataset en BigQuery\n",
        "- BUCKET_NAME: Nombre del bucket de Cloud Storage\n",
        "- Service Account con permisos de BigQuery y Cloud Storage\n",
        "\n",
        "## Archivos Requeridos:\n",
        "\n",
        "- `gs://[BUCKET_NAME]/raw-data/sri_vehiculos.csv`\n",
        "\n",
        "## Tablas Generadas:\n",
        "\n",
        "- `dim_tiempo`\n",
        "- `dim_vehiculo`\n",
        "- `dim_transaccion`\n",
        "- `dim_ubicacion`\n",
        "- `fact_registro_vehiculos`\n",
        "\"\"\"\n",
        "\n",
        "# Configurar tags adicionales para organización\n",
        "dag.tags.extend(['data-warehouse', 'gobierno', 'vehiculos'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dag.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "5Q7j0Q3Ha5v8",
        "outputId": "019dbb53-4e8e-4364-9992-4e98ccf543be"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "(sqlite3.OperationalError) no such table: task_instance\n[SQL: SELECT task_instance.try_number, task_instance.task_id, task_instance.dag_id, task_instance.run_id, task_instance.map_index, task_instance.start_date, task_instance.end_date, task_instance.duration, task_instance.state, task_instance.max_tries, task_instance.hostname, task_instance.unixname, task_instance.job_id, task_instance.pool, task_instance.pool_slots, task_instance.queue, task_instance.priority_weight, task_instance.operator, task_instance.custom_operator_name, task_instance.queued_dttm, task_instance.queued_by_job_id, task_instance.pid, task_instance.executor_config, task_instance.updated_at, task_instance.external_executor_id, task_instance.trigger_id, task_instance.trigger_timeout, task_instance.next_method, task_instance.next_kwargs, dag_run_1.state AS state_1, dag_run_1.id, dag_run_1.dag_id AS dag_id_1, dag_run_1.queued_at, dag_run_1.execution_date, dag_run_1.start_date AS start_date_1, dag_run_1.end_date AS end_date_1, dag_run_1.run_id AS run_id_1, dag_run_1.creating_job_id, dag_run_1.external_trigger, dag_run_1.run_type, dag_run_1.conf, dag_run_1.data_interval_start, dag_run_1.data_interval_end, dag_run_1.last_scheduling_decision, dag_run_1.dag_hash, dag_run_1.log_template_id, dag_run_1.updated_at AS updated_at_1 \nFROM task_instance JOIN dag_run ON dag_run.dag_id = task_instance.dag_id AND dag_run.run_id = task_instance.run_id JOIN dag_run AS dag_run_1 ON dag_run_1.dag_id = task_instance.dag_id AND dag_run_1.run_id = task_instance.run_id \nWHERE task_instance.dag_id = ? AND task_instance.task_id IN (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?) AND dag_run.execution_date >= ? AND dag_run.execution_date <= ? AND task_instance.operator = ?]\n[parameters: ('sri_vehiculos_etl_proceso', 'inicio_proceso_etl', 'etl_dim_tiempo', 'etl_dim_vehiculo', 'etl_dim_transaccion', 'etl_dim_ubicacion', 'sincronizacion_dimensiones', 'etl_fact_registro_vehiculos', 'finalizacion_proceso_etl', 'validar_calidad_datos', 'generar_metricas_negocio', 'notificar_finalizacion', '2025-07-02 05:07:54.590300', '2025-07-02 05:07:54.590300', 'ExternalTaskMarker')]\n(Background on this error at: https://sqlalche.me/e/14/e3q8)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1909\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m                     self.dialect.do_execute(\n\u001b[0m\u001b[1;32m   1911\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: no such table: task_instance",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35-949037698.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m     \u001b[0mdag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/airflow/utils/session.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/airflow/models/dag.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, execution_date, run_conf, conn_file_path, variable_file_path, session)\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clearing existing task instances for execution date %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2720\u001b[0;31m         self.clear(\n\u001b[0m\u001b[1;32m   2721\u001b[0m             \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m             \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/airflow/utils/session.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"session\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msession_args_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/airflow/models/dag.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self, task_ids, start_date, end_date, only_failed, only_running, confirm_prompt, include_subdags, include_parentdag, dag_run_state, dry_run, session, get_tis, recursion_depth, max_recursion_depth, dag_bag, exclude_task_ids)\u001b[0m\n\u001b[1;32m   2195\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTaskInstanceState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m         tis = self._get_task_instances(\n\u001b[0m\u001b[1;32m   2198\u001b[0m             \u001b[0mtask_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m             \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/airflow/models/dag.py\u001b[0m in \u001b[0;36m_get_task_instances\u001b[0;34m(self, task_ids, as_pk_tuple, start_date, end_date, run_id, state, include_subdags, include_parentdag, include_dependent_dags, exclude_task_ids, session, dag_bag, recursion_depth, max_recursion_depth, visited_external_tis)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mvisited_external_tis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0mexternal_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mExternalTaskMarker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mti\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexternal_tasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36mscalars\u001b[0;34m(self, statement, params, execution_options, bind_arguments, **kw)\u001b[0m\n\u001b[1;32m   1776\u001b[0m         \"\"\"\n\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m         return self.execute(\n\u001b[0m\u001b[1;32m   1779\u001b[0m             \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, statement, params, execution_options, bind_arguments, _parent_execute_state, _add_event, **kw)\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m             \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection_for_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_20\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompile_state_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_20\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1708\u001b[0m             )\n\u001b[1;32m   1709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_10style\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_10style\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m     def exec_driver_sql(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, multiparams, params, execution_options, _force)\u001b[0m\n\u001b[1;32m    332\u001b[0m     ):\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_force\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_execution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             return connection._execute_clauseelement(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, multiparams, params, execution_options)\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0mlinting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_linting\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN_LINTING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m         )\n\u001b[0;32m-> 1577\u001b[0;31m         ret = self._execute_context(\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_ctx_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_compiled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             self._handle_dbapi_exception(\n\u001b[0m\u001b[1;32m   1954\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   2132\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewraise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2134\u001b[0;31m                 util.raise_(\n\u001b[0m\u001b[1;32m   2135\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;31m# credit to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1908\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m                     self.dialect.do_execute(\n\u001b[0m\u001b[1;32m   1911\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such table: task_instance\n[SQL: SELECT task_instance.try_number, task_instance.task_id, task_instance.dag_id, task_instance.run_id, task_instance.map_index, task_instance.start_date, task_instance.end_date, task_instance.duration, task_instance.state, task_instance.max_tries, task_instance.hostname, task_instance.unixname, task_instance.job_id, task_instance.pool, task_instance.pool_slots, task_instance.queue, task_instance.priority_weight, task_instance.operator, task_instance.custom_operator_name, task_instance.queued_dttm, task_instance.queued_by_job_id, task_instance.pid, task_instance.executor_config, task_instance.updated_at, task_instance.external_executor_id, task_instance.trigger_id, task_instance.trigger_timeout, task_instance.next_method, task_instance.next_kwargs, dag_run_1.state AS state_1, dag_run_1.id, dag_run_1.dag_id AS dag_id_1, dag_run_1.queued_at, dag_run_1.execution_date, dag_run_1.start_date AS start_date_1, dag_run_1.end_date AS end_date_1, dag_run_1.run_id AS run_id_1, dag_run_1.creating_job_id, dag_run_1.external_trigger, dag_run_1.run_type, dag_run_1.conf, dag_run_1.data_interval_start, dag_run_1.data_interval_end, dag_run_1.last_scheduling_decision, dag_run_1.dag_hash, dag_run_1.log_template_id, dag_run_1.updated_at AS updated_at_1 \nFROM task_instance JOIN dag_run ON dag_run.dag_id = task_instance.dag_id AND dag_run.run_id = task_instance.run_id JOIN dag_run AS dag_run_1 ON dag_run_1.dag_id = task_instance.dag_id AND dag_run_1.run_id = task_instance.run_id \nWHERE task_instance.dag_id = ? AND task_instance.task_id IN (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?) AND dag_run.execution_date >= ? AND dag_run.execution_date <= ? AND task_instance.operator = ?]\n[parameters: ('sri_vehiculos_etl_proceso', 'inicio_proceso_etl', 'etl_dim_tiempo', 'etl_dim_vehiculo', 'etl_dim_transaccion', 'etl_dim_ubicacion', 'sincronizacion_dimensiones', 'etl_fact_registro_vehiculos', 'finalizacion_proceso_etl', 'validar_calidad_datos', 'generar_metricas_negocio', 'notificar_finalizacion', '2025-07-02 05:07:54.590300', '2025-07-02 05:07:54.590300', 'ExternalTaskMarker')]\n(Background on this error at: https://sqlalche.me/e/14/e3q8)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 9: Ejecutar ETL de dimensiones\n",
        "print(\"🚀 Ejecutando ETL de dimensiones...\")\n",
        "\n",
        "# Ejecutar cada dimensión\n",
        "try:\n",
        "    resultado_tiempo = etl_dim_tiempo()\n",
        "    print(f\"1️⃣ {resultado_tiempo}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error en dim_tiempo: {e}\")\n",
        "\n",
        "try:\n",
        "    resultado_vehiculo = etl_dim_vehiculo()\n",
        "    print(f\"2️⃣ {resultado_vehiculo}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error en dim_vehiculo: {e}\")\n",
        "\n",
        "try:\n",
        "    resultado_transaccion = etl_dim_transaccion()\n",
        "    print(f\"3️⃣ {resultado_transaccion}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error en dim_transaccion: {e}\")\n",
        "\n",
        "try:\n",
        "    resultado_ubicacion = etl_dim_ubicacion()\n",
        "    print(f\"4️⃣ {resultado_ubicacion}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error en dim_ubicacion: {e}\")\n",
        "\n",
        "print(\"✅ Dimensiones completadas\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awDo5jvvdzUu",
        "outputId": "7c19c064-8214-4996-c289-a7e6c036acf4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Ejecutando ETL de dimensiones...\n",
            "[\u001b[34m2025-07-02T05:09:08.525+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m56} INFO\u001b[0m - 🕐 Iniciando ETL para Dim_Tiempo...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:08.648+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m66} INFO\u001b[0m - 📅 Generando 2192 registros de fechas...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:15.092+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m116} INFO\u001b[0m - ✅ Cargados 2192 registros en dim_tiempo\u001b[0m\n",
            "1️⃣ Dim_Tiempo cargada exitosamente: 2192 registros\n",
            "[\u001b[34m2025-07-02T05:09:15.094+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m129} INFO\u001b[0m - 🚗 Iniciando ETL para Dim_Vehiculo...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:19.223+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m143} INFO\u001b[0m - 📊 Datos extraídos: 460550 registros originales\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:21.910+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m196} INFO\u001b[0m - 🔧 Transformación completada: 331160 vehículos únicos\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:27.789+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m205} INFO\u001b[0m - ✅ Cargados 331160 registros en dim_vehiculo\u001b[0m\n",
            "2️⃣ Dim_Vehiculo cargada exitosamente: 331160 registros\n",
            "[\u001b[34m2025-07-02T05:09:27.791+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m218} INFO\u001b[0m - 💼 Iniciando ETL para Dim_Transaccion...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:30.988+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m239} INFO\u001b[0m - Columnas encontradas: ['TIPO TRANSACCIÓN', 'TIPO SERVICIO', 'PERSONA NATURAL - JURÍDICA', 'CATEGORÍA']\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:31.122+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m267} INFO\u001b[0m - 🔧 Transformación completada: 8848 tipos de transacción únicos\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:36.345+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m276} INFO\u001b[0m - ✅ Cargados 8848 registros en dim_transaccion\u001b[0m\n",
            "3️⃣ Dim_Transaccion cargada exitosamente: 8848 registros\n",
            "[\u001b[34m2025-07-02T05:09:36.350+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m289} INFO\u001b[0m - 🌎 Iniciando ETL para Dim_Ubicacion...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:39.482+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m375} INFO\u001b[0m - 🔧 Transformación completada: 200 ubicaciones únicas\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:41.824+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m384} INFO\u001b[0m - ✅ Cargados 200 registros en dim_ubicacion\u001b[0m\n",
            "4️⃣ Dim_Ubicacion cargada exitosamente: 200 registros\n",
            "✅ Dimensiones completadas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 10: Ejecutar ETL de tabla de hechos\n",
        "print(\"📊 Ejecutando ETL de tabla de hechos...\")\n",
        "\n",
        "try:\n",
        "    resultado_fact = etl_fact_registro_vehiculos()\n",
        "    print(f\"✅ {resultado_fact}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error en fact table: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk_qFGzLin_Y",
        "outputId": "c146fb61-b06e-4849-c042-523bd776ec57"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Ejecutando ETL de tabla de hechos...\n",
            "[\u001b[34m2025-07-02T05:09:49.220+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m401} INFO\u001b[0m - 📊 Iniciando ETL para Fact_RegistroVehiculos...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:54.546+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m414} INFO\u001b[0m - 📊 Datos extraídos: 460550 registros de hechos\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:54.549+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m417} INFO\u001b[0m - 🔍 Cargando dimensiones para lookups...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:56.236+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m439} ERROR\u001b[0m - Error cargando dimensiones: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:56.239+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m586} ERROR\u001b[0m - ❌ Error en ETL Fact_RegistroVehiculos: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\u001b[0m\n",
            "❌ Error en fact table: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 11: Ejecutar validaciones y métricas\n",
        "print(\"🔍 Ejecutando validaciones...\")\n",
        "\n",
        "try:\n",
        "    validacion = validar_calidad_datos()\n",
        "    print(\"✅ Validación completada\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error en validación: {e}\")\n",
        "\n",
        "try:\n",
        "    metricas = generar_metricas_negocio()\n",
        "    print(\"✅ Métricas generadas\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error en métricas: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ai-Qz22ixbX",
        "outputId": "32d70a5f-8403-4411-8fd0-a3346ead7cd1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Ejecutando validaciones...\n",
            "[\u001b[34m2025-07-02T05:10:27.669+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m652} INFO\u001b[0m - 🔍 Iniciando validación de calidad de datos...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:10:28.926+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m759} ERROR\u001b[0m - ❌ Error en validación de calidad: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\u001b[0m\n",
            "❌ Error en validación: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n",
            "[\u001b[34m2025-07-02T05:10:28.928+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m767} INFO\u001b[0m - 📈 Generando métricas de negocio...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:10:29.308+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m843} ERROR\u001b[0m - ❌ Error generando métricas: 400 Name MontoAvaluo not found inside f at [5:19]; reason: invalidQuery, location: query, message: Name MontoAvaluo not found inside f at [5:19]\n",
            "\n",
            "Location: us-central1\n",
            "Job ID: 9592d064-8692-41c3-bcfd-fdff488b20e1\n",
            "\u001b[0m\n",
            "❌ Error en métricas: 400 Name MontoAvaluo not found inside f at [5:19]; reason: invalidQuery, location: query, message: Name MontoAvaluo not found inside f at [5:19]\n",
            "\n",
            "Location: us-central1\n",
            "Job ID: 9592d064-8692-41c3-bcfd-fdff488b20e1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 12: Verificar tablas creadas\n",
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "tablas_esperadas = ['dim_tiempo', 'dim_vehiculo', 'dim_transaccion', 'dim_ubicacion', 'fact_registro_vehiculos']\n",
        "\n",
        "print(\"📋 Verificando tablas creadas:\")\n",
        "for tabla in tablas_esperadas:\n",
        "    try:\n",
        "        query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{DATASET_ID}.{tabla}`\"\n",
        "        result = client.query(query).to_dataframe()\n",
        "        total = result.iloc[0]['total']\n",
        "        print(f\"✅ {tabla}: {total:,} registros\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ {tabla}: Error - {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wzBlRkYi3oW",
        "outputId": "487ca17c-91e3-458e-e8de-1ea1c95564b6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📋 Verificando tablas creadas:\n",
            "❌ dim_tiempo: Error - 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n",
            "❌ dim_vehiculo: Error - 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n",
            "❌ dim_transaccion: Error - 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n",
            "❌ dim_ubicacion: Error - 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n",
            "❌ fact_registro_vehiculos: Error - 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 13: Consulta de ejemplo del data warehouse\n",
        "query_ejemplo = f\"\"\"\n",
        "SELECT\n",
        "    t.Anio,\n",
        "    v.Marca,\n",
        "    u.Provincia,\n",
        "    COUNT(*) as total_registros,\n",
        "    AVG(f.MontoAvaluo) as avaluo_promedio\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos` f\n",
        "INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_tiempo` t ON f.ID_Tiempo = t.ID_Tiempo\n",
        "INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_vehiculo` v ON f.ID_Vehiculo = v.ID_Vehiculo\n",
        "INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_ubicacion` u ON f.ID_Ubicacion = u.ID_Ubicacion\n",
        "GROUP BY t.Anio, v.Marca, u.Provincia\n",
        "ORDER BY total_registros DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    resultado = client.query(query_ejemplo).to_dataframe()\n",
        "    print(\"📊 Top 10 combinaciones Año-Marca-Provincia:\")\n",
        "    print(resultado.to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error en consulta: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaVRMh3SjI4e",
        "outputId": "8377c2d5-7140-43f7-a580-4c5c8eb479a9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Error en consulta: 400 Name MontoAvaluo not found inside f at [7:11]; reason: invalidQuery, location: query, message: Name MontoAvaluo not found inside f at [7:11]\n",
            "\n",
            "Location: us-central1\n",
            "Job ID: b0f5d031-ddbd-45b1-bd36-7de68c443629\n",
            "\n"
          ]
        }
      ]
    }
  ]
}