{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2H9Ud2ZYJOu_"
      },
      "outputs": [],
      "source": [
        "# INSTRUCCIONES DE INSTALACI√ìN PARA GOOGLE COLAB:\n",
        "\"\"\"\n",
        "# Ejecutar en celdas separadas:\n",
        "\n",
        "# Celda 1: Instalaci√≥n de librer√≠as\n",
        "!pip install pandas openpyxl google-cloud-bigquery google-cloud-storage\n",
        "\n",
        "# Celda 2: Subir archivo de credenciales (opcional para simulaci√≥n)\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Solo ejecutar si tienes credenciales reales\n",
        "# uploaded = files.upload()\n",
        "# for filename in uploaded.keys():\n",
        "#     os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = filename\n",
        "\n",
        "# Celda 3: Ejecutar este c√≥digo completo\n",
        "\"\"\"\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "from io import StringIO\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 2: Configuraci√≥n inicial\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Subir el archivo JSON de la service account\n",
        "print(\"Sube tu archivo JSON de Service Account:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Configurar variable de entorno\n",
        "for filename in uploaded.keys():\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = filename\n",
        "    print(f\"Configurado: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "P1JwnpzZU0c7",
        "outputId": "fad16ed3-9629-44eb-fa04-e3b74c40cc41"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sube tu archivo JSON de Service Account:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1098044d-644f-44e5-8d11-d22d05b745b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1098044d-644f-44e5-8d11-d22d05b745b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving sri-vehiculos-etl-6e14a22d7578.json to sri-vehiculos-etl-6e14a22d7578 (1).json\n",
            "Configurado: sri-vehiculos-etl-6e14a22d7578 (1).json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONFIGURACI√ìN Y CLASES MOCK\n",
        "# ============================================================================\n",
        "\n",
        "# Variables de configuraci√≥n\n",
        "PROJECT_ID = 'sri-vehiculos-etl'\n",
        "DATASET_ID = 'sri_vehiculos_dw'\n",
        "BUCKET_NAME = 'sri-vehiculos-etl-bucket-angel'\n",
        "\n",
        "print(f\"Configuraci√≥n:\")\n",
        "print(f\"- Proyecto: {PROJECT_ID}\")\n",
        "print(f\"- Dataset: {DATASET_ID}\")\n",
        "print(f\"- Bucket: {BUCKET_NAME}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHgMk0RjK7r8",
        "outputId": "939615ea-08c5-4853-d8f0-ed77023dccaf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuraci√≥n:\n",
            "- Proyecto: sri-vehiculos-etl\n",
            "- Dataset: sri_vehiculos_dw\n",
            "- Bucket: sri-vehiculos-etl-bucket-angel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 4: Crear dataset en BigQuery\n",
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Crear dataset si no existe\n",
        "dataset_ref = client.dataset(DATASET_ID)\n",
        "try:\n",
        "    client.get_dataset(dataset_ref)\n",
        "    print(f\"‚úÖ Dataset {DATASET_ID} ya existe\")\n",
        "except:\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"US\"  # o \"EU\" seg√∫n tu preferencia\n",
        "    dataset = client.create_dataset(dataset)\n",
        "    print(f\"‚úÖ Dataset {DATASET_ID} creado exitosamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzoz9SlcLV-8",
        "outputId": "2e922a6f-565e-4f8b-ba2c-b4c096728406"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset sri_vehiculos_dw ya existe\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Celda 5: Crear bucket de Cloud Storage\n",
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client()\n",
        "BUCKET_NAME = \"sri-vehiculos-etl-bucket-angel\"\n",
        "\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "# Subimos un archivo de prueba\n",
        "blob = bucket.blob(\"raw-data/.keep\")\n",
        "blob.upload_from_string('')\n",
        "print(\"‚úÖ Carpeta raw-data/ creada\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-asDS-QXLrnW",
        "outputId": "670349fa-629e-46c3-9aa7-c422ad64966b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Carpeta raw-data/ creada\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 6: Subir archivo de datos SRI\n",
        "from google.colab import files\n",
        "from google.cloud import storage\n",
        "import io\n",
        "\n",
        "print(\"Sube tu archivo CSV de datos del SRI:\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "# Subir al bucket de Cloud Storage\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "for filename, content in uploaded_files.items():\n",
        "    # Subir a la carpeta raw-data\n",
        "    blob = bucket.blob(f'raw-data/sri_vehiculos.csv')\n",
        "    blob.upload_from_string(content.decode('utf-8'))\n",
        "    print(f\"‚úÖ Archivo {filename} subido como sri_vehiculos.csv\")\n",
        "\n",
        "    # Mostrar informaci√≥n del archivo\n",
        "    print(f\"üìä Tama√±o del archivo: {len(content)} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "JCRLnBjRYLIW",
        "outputId": "98e97ef3-4f00-4013-9ea8-2b3c5200b44b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sube tu archivo CSV de datos del SRI:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-32fcf6ac-c8e2-43fe-960d-26fa252b83cc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-32fcf6ac-c8e2-43fe-960d-26fa252b83cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving SRI-vehiculos - SRI_Vehiculos_Nuevos.csv to SRI-vehiculos - SRI_Vehiculos_Nuevos.csv\n",
            "‚úÖ Archivo SRI-vehiculos - SRI_Vehiculos_Nuevos.csv subido como sri_vehiculos.csv\n",
            "üìä Tama√±o del archivo: 75189919 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 7: Verificar configuraci√≥n\n",
        "from google.cloud import bigquery, storage\n",
        "import pandas as pd\n",
        "\n",
        "print(\"üîç Verificando configuraci√≥n...\")\n",
        "\n",
        "# Verificar BigQuery\n",
        "try:\n",
        "    bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "    datasets = list(bq_client.list_datasets())\n",
        "    print(f\"‚úÖ BigQuery conectado. Datasets disponibles: {len(datasets)}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error BigQuery: {e}\")\n",
        "\n",
        "# Verificar Cloud Storage\n",
        "try:\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(BUCKET_NAME)\n",
        "    blobs = list(bucket.list_blobs(prefix='raw-data/'))\n",
        "    print(f\"‚úÖ Cloud Storage conectado. Archivos en raw-data/: {len(blobs)}\")\n",
        "    for blob in blobs:\n",
        "        print(f\"   - {blob.name}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error Cloud Storage: {e}\")\n",
        "\n",
        "# Verificar archivo CSV\n",
        "try:\n",
        "    blob = bucket.blob('raw-data/sri_vehiculos.csv')\n",
        "    content = blob.download_as_text()\n",
        "    df = pd.read_csv(io.StringIO(content))\n",
        "    print(f\"‚úÖ Archivo CSV le√≠do correctamente\")\n",
        "    print(f\"   - Filas: {len(df)}\")\n",
        "    print(f\"   - Columnas: {len(df.columns)}\")\n",
        "    print(f\"   - Columnas disponibles: {list(df.columns[:10])}\")  # Primeras 10\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error leyendo CSV: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oiVq7lhaWiV",
        "outputId": "dd66a38d-6587-44ec-f1b5-e40b8c53861f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Verificando configuraci√≥n...\n",
            "‚úÖ BigQuery conectado. Datasets disponibles: 1\n",
            "‚úÖ Cloud Storage conectado. Archivos en raw-data/: 4\n",
            "   - raw-data/\n",
            "   - raw-data/.keep\n",
            "   - raw-data/SRI-vehiculos.xlsx\n",
            "   - raw-data/sri_vehiculos.csv\n",
            "‚úÖ Archivo CSV le√≠do correctamente\n",
            "   - Filas: 460550\n",
            "   - Columnas: 20\n",
            "   - Columnas disponibles: ['CATEGOR√çA', 'C√ìDIGO DE VEH√çCULO', 'TIPO TRANSACCI√ìN', 'MARCA', 'MODELO', 'PA√çS', 'A√ëO MODELO', 'CLASE', 'SUB CLASE', 'TIPO']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Soluci√≥n sin reinstalar ni reiniciar - Parche directo al m√≥dulo\n",
        "import sys\n",
        "import pendulum\n",
        "\n",
        "# Crear el m√©todo que Airflow espera encontrar\n",
        "if not hasattr(pendulum.tz, 'timezone') or not callable(pendulum.tz.timezone):\n",
        "    # Parche: crear la funci√≥n que Airflow 2.7.0 busca\n",
        "    def timezone_function(tz_name):\n",
        "        return pendulum.timezone(tz_name)\n",
        "\n",
        "    # Aplicar el parche\n",
        "    pendulum.tz.timezone = timezone_function\n",
        "    print(\"‚úÖ Parche aplicado a pendulum.tz.timezone\")\n",
        "\n",
        "# Tambi√©n parchear el m√≥dulo ya cargado en sys.modules si existe\n",
        "if 'pendulum.tz' in sys.modules:\n",
        "    sys.modules['pendulum.tz'].timezone = timezone_function\n",
        "    print(\"‚úÖ Parche aplicado al m√≥dulo cargado\")\n",
        "\n",
        "# Limpiar solo los m√≥dulos de airflow (no pendulum)\n",
        "airflow_modules = [mod for mod in sys.modules if 'airflow' in mod]\n",
        "for mod in airflow_modules:\n",
        "    del sys.modules[mod]\n",
        "print(f\"‚úÖ {len(airflow_modules)} m√≥dulos de airflow limpiados\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtwaMUIWgPvX",
        "outputId": "896b046b-6627-4113-9d33-5af0fc1643fd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Parche aplicado a pendulum.tz.timezone\n",
            "‚úÖ Parche aplicado al m√≥dulo cargado\n",
            "‚úÖ 42 m√≥dulos de airflow limpiados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLUCI√ìN DEFINITIVA - Deshabilitar BD de Airflow completamente\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Variables de entorno para deshabilitar funciones que requieren BD\n",
        "os.environ['AIRFLOW__CORE__UNIT_TEST_MODE'] = 'True'\n",
        "os.environ['AIRFLOW__CORE__LOAD_EXAMPLES'] = 'False'\n",
        "os.environ['AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS'] = 'False'\n",
        "os.environ['AIRFLOW__CORE__SQL_ALCHEMY_CONN'] = 'sqlite:///:memory:'\n",
        "os.environ['AIRFLOW__WEBSERVER__EXPOSE_CONFIG'] = 'False'\n",
        "\n",
        "# Configurar logging para suprimir errores de BD\n",
        "import logging\n",
        "logging.getLogger('airflow').setLevel(logging.CRITICAL)\n",
        "logging.getLogger('sqlalchemy').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Parche para evitar queries a task_instance\n",
        "from unittest.mock import patch, MagicMock\n",
        "\n",
        "# Mock del DAG para evitar queries a BD\n",
        "original_dag_init = None\n",
        "\n",
        "def mock_dag_init(self, *args, **kwargs):\n",
        "    # Llamar constructor original pero sin validaciones de BD\n",
        "    try:\n",
        "        if original_dag_init:\n",
        "            original_dag_init(self, *args, **kwargs)\n",
        "    except:\n",
        "        # Si falla, crear DAG m√≠nimo\n",
        "        self.dag_id = kwargs.get('dag_id', args[0] if args else 'default')\n",
        "        self.default_args = kwargs.get('default_args', {})\n",
        "        self.tasks = []\n",
        "        self.task_dict = {}\n",
        "        self._task_group = None\n",
        "\n",
        "# Importar y patchear\n",
        "from airflow import DAG as OriginalDAG\n",
        "original_dag_init = OriginalDAG.__init__\n",
        "\n",
        "# Aplicar patch\n",
        "OriginalDAG.__init__ = mock_dag_init\n",
        "\n",
        "print(\"‚úÖ Airflow configurado para modo desarrollo sin BD\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiEnLeC7hSx9",
        "outputId": "f8547b2e-bb7c-41b5-ad3b-37b07703762e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Airflow configurado para modo desarrollo sin BD\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SOLUCI√ìN DEFINITIVA: Airflow Mock para Google Colab\n",
        "# Sin dependencias de base de datos, completamente funcional para desarrollo\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from google.cloud import storage, bigquery\n",
        "import hashlib\n",
        "from io import StringIO\n",
        "import logging\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class MockDAG:\n",
        "    \"\"\"Simulador de DAG de Airflow sin dependencias de BD\"\"\"\n",
        "\n",
        "    def __init__(self, dag_id, default_args=None, description=None,\n",
        "                 schedule_interval=None, start_date=None, catchup=False,\n",
        "                 tags=None, **kwargs):\n",
        "        self.dag_id = dag_id\n",
        "        self.default_args = default_args or {}\n",
        "        self.description = description\n",
        "        self.schedule_interval = schedule_interval\n",
        "        self.start_date = start_date\n",
        "        self.catchup = catchup\n",
        "        self.tags = tags or []\n",
        "        self.tasks = []\n",
        "        self.task_dict = {}\n",
        "        self._current_dag = None\n",
        "\n",
        "        logger.info(f\"üîß DAG '{dag_id}' creado exitosamente\")\n",
        "\n",
        "    def __enter__(self):\n",
        "        MockDAG._current_dag = self\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        MockDAG._current_dag = None\n",
        "        logger.info(f\"‚úÖ DAG '{self.dag_id}' configurado con {len(self.tasks)} tareas\")\n",
        "\n",
        "    def add_task(self, task):\n",
        "        \"\"\"Agregar tarea al DAG\"\"\"\n",
        "        self.tasks.append(task)\n",
        "        self.task_dict[task.task_id] = task\n",
        "        task.dag = self\n",
        "\n",
        "    def run_all_tasks(self):\n",
        "        \"\"\"Ejecutar todas las tareas del DAG secuencialmente\"\"\"\n",
        "        logger.info(f\"üöÄ Iniciando ejecuci√≥n del DAG: {self.dag_id}\")\n",
        "        results = {}\n",
        "\n",
        "        for task in self.tasks:\n",
        "            try:\n",
        "                logger.info(f\"‚ñ∂Ô∏è  Ejecutando tarea: {task.task_id}\")\n",
        "                result = task.execute()\n",
        "                results[task.task_id] = result\n",
        "                logger.info(f\"‚úÖ Tarea {task.task_id} completada\")\n",
        "                if result:\n",
        "                    print(f\"   üìä Resultado: {result}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"‚ùå Error en tarea {task.task_id}: {str(e)}\")\n",
        "                results[task.task_id] = f\"ERROR: {str(e)}\"\n",
        "\n",
        "        logger.info(f\"üéâ DAG {self.dag_id} completado\")\n",
        "        return results\n",
        "\n",
        "\n",
        "class MockPythonOperator:\n",
        "    \"\"\"Simulador de PythonOperator sin dependencias de BD\"\"\"\n",
        "\n",
        "    def __init__(self, task_id, python_callable=None, op_args=None,\n",
        "                 op_kwargs=None, dag=None, **kwargs):\n",
        "        self.task_id = task_id\n",
        "        self.python_callable = python_callable\n",
        "        self.op_args = op_args or []\n",
        "        self.op_kwargs = op_kwargs or {}\n",
        "        self.dag = dag\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "        # Auto-agregar al DAG actual si existe\n",
        "        if dag:\n",
        "            dag.add_task(self)\n",
        "        elif MockDAG._current_dag:\n",
        "            MockDAG._current_dag.add_task(self)\n",
        "\n",
        "        logger.info(f\"üîß PythonOperator '{task_id}' registrado\")\n",
        "\n",
        "    def execute(self):\n",
        "        \"\"\"Ejecutar la funci√≥n Python\"\"\"\n",
        "        if self.python_callable:\n",
        "            try:\n",
        "                # Crear contexto mock\n",
        "                context = {\n",
        "                    'dag': self.dag,\n",
        "                    'task': self,\n",
        "                    'task_instance': self,\n",
        "                    'execution_date': datetime.now(),\n",
        "                    'ds': datetime.now().strftime('%Y-%m-%d'),\n",
        "                    'ts': datetime.now().isoformat(),\n",
        "                }\n",
        "\n",
        "                # Ejecutar funci√≥n con argumentos\n",
        "                if self.op_kwargs:\n",
        "                    return self.python_callable(*self.op_args, **self.op_kwargs, **context)\n",
        "                else:\n",
        "                    return self.python_callable(*self.op_args, **context)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error ejecutando {self.task_id}: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            logger.info(f\"Tarea {self.task_id} sin funci√≥n asociada\")\n",
        "            return None\n",
        "\n",
        "\n",
        "class MockEmptyOperator:\n",
        "    \"\"\"Simulador de EmptyOperator (antes DummyOperator)\"\"\"\n",
        "\n",
        "    def __init__(self, task_id, dag=None, **kwargs):\n",
        "        self.task_id = task_id\n",
        "        self.dag = dag\n",
        "        self.kwargs = kwargs\n",
        "\n",
        "        # Auto-agregar al DAG actual si existe\n",
        "        if dag:\n",
        "            dag.add_task(self)\n",
        "        elif MockDAG._current_dag:\n",
        "            MockDAG._current_dag.add_task(self)\n",
        "\n",
        "        logger.info(f\"‚≠ï EmptyOperator '{task_id}' registrado\")\n",
        "\n",
        "    def execute(self):\n",
        "        \"\"\"Simular ejecuci√≥n vac√≠a\"\"\"\n",
        "        logger.info(f\"‚è≠Ô∏è  Ejecutando tarea vac√≠a: {self.task_id}\")\n",
        "        return f\"Tarea {self.task_id} ejecutada exitosamente\"\n",
        "\n",
        "\n",
        "# Funciones de utilidad para testing\n",
        "def ejecutar_tarea_individual(dag, task_id):\n",
        "    \"\"\"Ejecutar una tarea espec√≠fica del DAG\"\"\"\n",
        "    if task_id in dag.task_dict:\n",
        "        task = dag.task_dict[task_id]\n",
        "        logger.info(f\"üéØ Ejecutando tarea individual: {task_id}\")\n",
        "        return task.execute()\n",
        "    else:\n",
        "        logger.error(f\"‚ùå Tarea '{task_id}' no encontrada en el DAG\")\n",
        "        return None\n",
        "\n",
        "def listar_tareas_dag(dag):\n",
        "    \"\"\"Mostrar todas las tareas de un DAG\"\"\"\n",
        "    print(f\"\\nüìã Tareas en DAG '{dag.dag_id}':\")\n",
        "    for i, task in enumerate(dag.tasks, 1):\n",
        "        task_type = \"Python\" if hasattr(task, 'python_callable') else \"Empty\"\n",
        "        print(f\"  {i}. {task.task_id} ({task_type})\")\n",
        "    print()\n",
        "\n",
        "def validar_estructura_dag(dag):\n",
        "    \"\"\"Validar que el DAG est√© bien estructurado\"\"\"\n",
        "    print(f\"\\nüîç Validando DAG: {dag.dag_id}\")\n",
        "    print(f\"  ‚úì N√∫mero de tareas: {len(dag.tasks)}\")\n",
        "    print(f\"  ‚úì Schedule: {dag.schedule_interval}\")\n",
        "    print(f\"  ‚úì Start date: {dag.start_date}\")\n",
        "    print(f\"  ‚úì Tags: {dag.tags}\")\n",
        "\n",
        "    # Validar tareas Python\n",
        "    python_tasks = [t for t in dag.tasks if hasattr(t, 'python_callable')]\n",
        "    empty_tasks = [t for t in dag.tasks if not hasattr(t, 'python_callable')]\n",
        "\n",
        "    print(f\"  ‚úì Tareas Python: {len(python_tasks)}\")\n",
        "    print(f\"  ‚úì Tareas Empty: {len(empty_tasks)}\")\n",
        "\n",
        "    # Verificar funciones Python\n",
        "    missing_functions = [t.task_id for t in python_tasks if t.python_callable is None]\n",
        "    if missing_functions:\n",
        "        print(f\"  ‚ö†Ô∏è  Tareas sin funci√≥n: {missing_functions}\")\n",
        "    else:\n",
        "        print(\"  ‚úÖ Todas las tareas Python tienen funciones asignadas\")\n",
        "\n",
        "    print(\"‚úÖ Validaci√≥n completada\\n\")\n",
        "\n",
        "# Crear aliases para compatibilidad con c√≥digo Airflow existente\n",
        "DAG = MockDAG\n",
        "PythonOperator = MockPythonOperator\n",
        "EmptyOperator = MockEmptyOperator\n",
        "DummyOperator = MockEmptyOperator  # Alias para compatibilidad\n",
        "\n",
        "# Variable global para DAG actual\n",
        "MockDAG._current_dag = None\n",
        "\n",
        "print(\"üéâ ¬°Airflow Mock cargado exitosamente!\")\n",
        "print(\"‚úÖ Puedes usar DAG, PythonOperator y EmptyOperator normalmente\")\n",
        "print(\"üß™ Funciones adicionales:\")\n",
        "print(\"   - dag.run_all_tasks() - Ejecutar todo el DAG\")\n",
        "print(\"   - ejecutar_tarea_individual(dag, 'task_id') - Ejecutar una tarea\")\n",
        "print(\"   - listar_tareas_dag(dag) - Ver todas las tareas\")\n",
        "print(\"   - validar_estructura_dag(dag) - Validar el DAG\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2vuI7pfiKj-",
        "outputId": "9bf858ca-6a97-4255-9f40-316c4694f3c5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéâ ¬°Airflow Mock cargado exitosamente!\n",
            "‚úÖ Puedes usar DAG, PythonOperator y EmptyOperator normalmente\n",
            "üß™ Funciones adicionales:\n",
            "   - dag.run_all_tasks() - Ejecutar todo el DAG\n",
            "   - ejecutar_tarea_individual(dag, 'task_id') - Ejecutar una tarea\n",
            "   - listar_tareas_dag(dag) - Ver todas las tareas\n",
            "   - validar_estructura_dag(dag) - Validar el DAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SRI_Vehiculos_ETL_DAG.py\n",
        "# Implementaci√≥n completa del proceso ETL para datos vehiculares del SRI\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.python import PythonOperator\n",
        "from airflow.operators.empty import EmptyOperator\n",
        "import pandas as pd\n",
        "from google.cloud import storage, bigquery\n",
        "import hashlib\n",
        "from io import StringIO\n",
        "import logging\n",
        "DummyOperator = EmptyOperator\n",
        "\n",
        "\n",
        "# Configuraci√≥n por defecto del DAG\n",
        "default_args = {\n",
        "    'owner': 'sri_data_engineer',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 1, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 2,\n",
        "    'retry_delay': timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "# Definici√≥n del DAG\n",
        "dag = DAG(\n",
        "    'sri_vehiculos_etl_proceso',\n",
        "    default_args=default_args,\n",
        "    description='Proceso ETL completo para datos vehiculares del SRI',\n",
        "    schedule_interval='@daily',\n",
        "    catchup=False,\n",
        "    tags=['sri', 'vehiculos', 'etl', 'bigquery'],\n",
        "    max_active_runs=1\n",
        ")\n",
        "\n",
        "# Variables de configuraci√≥n\n",
        "PROJECT_ID = 'sri-vehiculos-etl'  # Reemplazar con tu project ID\n",
        "DATASET_ID = 'sri_vehiculos_dw'\n",
        "BUCKET_NAME = 'sri-vehiculos-etl-bucket-angel'  # Reemplazar con tu bucket\n",
        "\n",
        "# ===============================\n",
        "# FUNCIONES ETL PARA DIMENSIONES\n",
        "# ===============================\n",
        "\n",
        "def etl_dim_tiempo(**context):\n",
        "    \"\"\"\n",
        "    Proceso ETL para la dimensi√≥n Tiempo\n",
        "    Genera un rango completo de fechas desde 2020 hasta 2025\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"üïê Iniciando ETL para Dim_Tiempo...\")\n",
        "\n",
        "        # Configurar cliente de BigQuery\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Generar rango de fechas\n",
        "        start_date = datetime(2020, 1, 1)\n",
        "        end_date = datetime(2025, 12, 31)\n",
        "        fechas = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "        logging.info(f\"üìÖ Generando {len(fechas)} registros de fechas...\")\n",
        "\n",
        "        # Crear DataFrame de dimensi√≥n tiempo\n",
        "        dim_tiempo = pd.DataFrame({\n",
        "            'ID_Tiempo': range(1, len(fechas) + 1),\n",
        "            'FechaCompleta': fechas.date,\n",
        "            'Anio': fechas.year,\n",
        "            'Trimestre': fechas.quarter,\n",
        "            'Mes': fechas.month,\n",
        "            'Dia': fechas.day,\n",
        "            'NombreMes': fechas.strftime('%B'),\n",
        "            'NombreDiaSemana': fechas.strftime('%A')\n",
        "        })\n",
        "\n",
        "        # Traducir nombres al espa√±ol\n",
        "        meses_es = {\n",
        "            'January': 'Enero', 'February': 'Febrero', 'March': 'Marzo',\n",
        "            'April': 'Abril', 'May': 'Mayo', 'June': 'Junio',\n",
        "            'July': 'Julio', 'August': 'Agosto', 'September': 'Septiembre',\n",
        "            'October': 'Octubre', 'November': 'Noviembre', 'December': 'Diciembre'\n",
        "        }\n",
        "\n",
        "        dias_es = {\n",
        "            'Monday': 'Lunes', 'Tuesday': 'Martes', 'Wednesday': 'Mi√©rcoles',\n",
        "            'Thursday': 'Jueves', 'Friday': 'Viernes', 'Saturday': 'S√°bado',\n",
        "            'Sunday': 'Domingo'\n",
        "        }\n",
        "\n",
        "        dim_tiempo['NombreMes'] = dim_tiempo['NombreMes'].map(meses_es)\n",
        "        dim_tiempo['NombreDiaSemana'] = dim_tiempo['NombreDiaSemana'].map(dias_es)\n",
        "\n",
        "        # Cargar a BigQuery\n",
        "        table_id = f'{PROJECT_ID}.{DATASET_ID}.dim_tiempo'\n",
        "        job_config = bigquery.LoadJobConfig(\n",
        "            write_disposition=\"WRITE_TRUNCATE\",\n",
        "            schema=[\n",
        "                bigquery.SchemaField(\"ID_Tiempo\", \"INTEGER\"),\n",
        "                bigquery.SchemaField(\"FechaCompleta\", \"DATE\"),\n",
        "                bigquery.SchemaField(\"Anio\", \"INTEGER\"),\n",
        "                bigquery.SchemaField(\"Trimestre\", \"INTEGER\"),\n",
        "                bigquery.SchemaField(\"Mes\", \"INTEGER\"),\n",
        "                bigquery.SchemaField(\"Dia\", \"INTEGER\"),\n",
        "                bigquery.SchemaField(\"NombreMes\", \"STRING\"),\n",
        "                bigquery.SchemaField(\"NombreDiaSemana\", \"STRING\"),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        job = client.load_table_from_dataframe(dim_tiempo, table_id, job_config=job_config)\n",
        "        job.result()  # Esperar a que termine\n",
        "\n",
        "        logging.info(f\"‚úÖ Cargados {len(dim_tiempo)} registros en dim_tiempo\")\n",
        "        return f\"Dim_Tiempo cargada exitosamente: {len(dim_tiempo)} registros\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Error en ETL Dim_Tiempo: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def etl_dim_vehiculo(**context):\n",
        "    \"\"\"\n",
        "    Proceso ETL para la dimensi√≥n Veh√≠culo\n",
        "    Extrae caracter√≠sticas √∫nicas de veh√≠culos del archivo CSV\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"üöó Iniciando ETL para Dim_Vehiculo...\")\n",
        "\n",
        "        # Configurar clientes\n",
        "        storage_client = storage.Client()\n",
        "        bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Extraer datos del bucket\n",
        "        bucket = storage_client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob('raw-data/sri_vehiculos.csv')\n",
        "\n",
        "        # Leer CSV desde Cloud Storage\n",
        "        content = blob.download_as_text()\n",
        "        df = pd.read_csv(StringIO(content))\n",
        "\n",
        "        logging.info(f\"üìä Datos extra√≠dos: {len(df)} registros originales\")\n",
        "\n",
        "        # Seleccionar columnas para la dimensi√≥n veh√≠culo\n",
        "        columnas_vehiculo = [\n",
        "            'C√ìDIGO DE VEH√çCULO', 'MARCA', 'MODELO', 'PA√çS',\n",
        "            'A√ëO MODELO', 'CLASE', 'SUB CLASE', 'TIPO',\n",
        "            'CILINDRAJE', 'TIPO COMBUSTIBLE', 'COLOR 1', 'COLOR 2'\n",
        "        ]\n",
        "\n",
        "        # Verificar que las columnas existen\n",
        "        columnas_existentes = [col for col in columnas_vehiculo if col in df.columns]\n",
        "        if len(columnas_existentes) != len(columnas_vehiculo):\n",
        "            logging.warning(f\"Algunas columnas no encontradas. Usando: {columnas_existentes}\")\n",
        "\n",
        "        # Crear dimensi√≥n con registros √∫nicos\n",
        "        dim_vehiculo = df[columnas_existentes].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "        # Generar clave subrogada\n",
        "        dim_vehiculo['ID_Vehiculo'] = range(1, len(dim_vehiculo) + 1)\n",
        "\n",
        "        # Limpiar y estandarizar datos\n",
        "        for col in ['MARCA', 'MODELO', 'PA√çS', 'CLASE', 'SUB CLASE', 'TIPO', 'TIPO COMBUSTIBLE']:\n",
        "            if col in dim_vehiculo.columns:\n",
        "                dim_vehiculo[col] = dim_vehiculo[col].astype(str).str.upper().str.strip()\n",
        "\n",
        "        # Manejar valores nulos\n",
        "        if 'COLOR 2' in dim_vehiculo.columns:\n",
        "            dim_vehiculo['COLOR 2'] = dim_vehiculo['COLOR 2'].fillna('N/A')\n",
        "\n",
        "        # Renombrar columnas para BigQuery (sin espacios ni caracteres especiales)\n",
        "        rename_dict = {\n",
        "            'C√ìDIGO DE VEH√çCULO': 'CodigoVehiculo',\n",
        "            'MARCA': 'Marca',\n",
        "            'MODELO': 'Modelo',\n",
        "            'PA√çS': 'Pais',\n",
        "            'A√ëO MODELO': 'AnioModelo',\n",
        "            'CLASE': 'Clase',\n",
        "            'SUB CLASE': 'SubClase',\n",
        "            'TIPO': 'Tipo',\n",
        "            'CILINDRAJE': 'Cilindraje',\n",
        "            'TIPO COMBUSTIBLE': 'TipoCombustible',\n",
        "            'COLOR 1': 'Color1',\n",
        "            'COLOR 2': 'Color2'\n",
        "        }\n",
        "\n",
        "        # Solo renombrar columnas que existen\n",
        "        rename_dict_filtered = {k: v for k, v in rename_dict.items() if k in dim_vehiculo.columns}\n",
        "        dim_vehiculo = dim_vehiculo.rename(columns=rename_dict_filtered)\n",
        "\n",
        "        # Reordenar columnas (solo las que existen)\n",
        "        columnas_orden = ['ID_Vehiculo'] + [v for k, v in rename_dict_filtered.items()]\n",
        "        dim_vehiculo = dim_vehiculo[columnas_orden]\n",
        "\n",
        "        logging.info(f\"üîß Transformaci√≥n completada: {len(dim_vehiculo)} veh√≠culos √∫nicos\")\n",
        "\n",
        "        # Cargar a BigQuery\n",
        "        table_id = f'{PROJECT_ID}.{DATASET_ID}.dim_vehiculo'\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "        job = bigquery_client.load_table_from_dataframe(dim_vehiculo, table_id, job_config=job_config)\n",
        "        job.result()\n",
        "\n",
        "        logging.info(f\"‚úÖ Cargados {len(dim_vehiculo)} registros en dim_vehiculo\")\n",
        "        return f\"Dim_Vehiculo cargada exitosamente: {len(dim_vehiculo)} registros\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Error en ETL Dim_Vehiculo: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def etl_dim_transaccion(**context):\n",
        "    \"\"\"\n",
        "    Proceso ETL para la dimensi√≥n Transacci√≥n\n",
        "    Crea combinaciones √∫nicas de tipos de transacci√≥n y servicio\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"üíº Iniciando ETL para Dim_Transaccion...\")\n",
        "\n",
        "        # Configurar clientes\n",
        "        storage_client = storage.Client()\n",
        "        bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Extraer datos del bucket\n",
        "        bucket = storage_client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob('raw-data/sri_vehiculos.csv')\n",
        "\n",
        "        content = blob.download_as_text()\n",
        "        df = pd.read_csv(StringIO(content))\n",
        "\n",
        "        # Seleccionar columnas para dimensi√≥n transacci√≥n\n",
        "        columnas_transaccion = [\n",
        "            'TIPO TRANSACCI√ìN', 'TIPO SERVICIO',\n",
        "            'PERSONA NATURAL - JUR√çDICA', 'CATEGOR√çA'\n",
        "        ]\n",
        "\n",
        "        # Verificar columnas existentes\n",
        "        columnas_existentes = [col for col in columnas_transaccion if col in df.columns]\n",
        "        logging.info(f\"Columnas encontradas: {columnas_existentes}\")\n",
        "\n",
        "        # Crear dimensi√≥n con combinaciones √∫nicas\n",
        "        dim_transaccion = df[columnas_existentes].drop_duplicates().reset_index(drop=True)\n",
        "\n",
        "        # Generar clave subrogada\n",
        "        dim_transaccion['ID_Transaccion'] = range(1, len(dim_transaccion) + 1)\n",
        "\n",
        "        # Limpiar datos\n",
        "        for col in columnas_existentes:\n",
        "            if col in dim_transaccion.columns:\n",
        "                dim_transaccion[col] = dim_transaccion[col].astype(str).str.upper().str.strip()\n",
        "\n",
        "        # Renombrar columnas\n",
        "        rename_dict = {\n",
        "            'TIPO TRANSACCI√ìN': 'TipoTransaccion',\n",
        "            'TIPO SERVICIO': 'TipoServicio',\n",
        "            'PERSONA NATURAL - JUR√çDICA': 'PersonaTipo',\n",
        "            'CATEGOR√çA': 'Categoria'\n",
        "        }\n",
        "\n",
        "        rename_dict_filtered = {k: v for k, v in rename_dict.items() if k in dim_transaccion.columns}\n",
        "        dim_transaccion = dim_transaccion.rename(columns=rename_dict_filtered)\n",
        "\n",
        "        # Reordenar columnas\n",
        "        columnas_orden = ['ID_Transaccion'] + [v for k, v in rename_dict_filtered.items()]\n",
        "        dim_transaccion = dim_transaccion[columnas_orden]\n",
        "\n",
        "        logging.info(f\"üîß Transformaci√≥n completada: {len(dim_transaccion)} tipos de transacci√≥n √∫nicos\")\n",
        "\n",
        "        # Cargar a BigQuery\n",
        "        table_id = f'{PROJECT_ID}.{DATASET_ID}.dim_transaccion'\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "        job = bigquery_client.load_table_from_dataframe(dim_transaccion, table_id, job_config=job_config)\n",
        "        job.result()\n",
        "\n",
        "        logging.info(f\"‚úÖ Cargados {len(dim_transaccion)} registros en dim_transaccion\")\n",
        "        return f\"Dim_Transaccion cargada exitosamente: {len(dim_transaccion)} registros\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Error en ETL Dim_Transaccion: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def etl_dim_ubicacion(**context):\n",
        "    \"\"\"\n",
        "    Proceso ETL para la dimensi√≥n Ubicaci√≥n\n",
        "    Mapea c√≥digos de cant√≥n a informaci√≥n geogr√°fica completa\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"üåé Iniciando ETL para Dim_Ubicacion...\")\n",
        "\n",
        "        # Configurar clientes\n",
        "        storage_client = storage.Client()\n",
        "        bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Extraer datos del bucket\n",
        "        bucket = storage_client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob('raw-data/sri_vehiculos.csv')\n",
        "\n",
        "        content = blob.download_as_text()\n",
        "        df = pd.read_csv(StringIO(content))\n",
        "\n",
        "        # Mapeo de cantones expandido\n",
        "        mapeo_cantones = {\n",
        "            '10701': {'canton': 'CUENCA', 'provincia': 'AZUAY', 'region': 'SIERRA'},\n",
        "            '10911': {'canton': 'GIRON', 'provincia': 'AZUAY', 'region': 'SIERRA'},\n",
        "            '10901': {'canton': 'GUALACEO', 'provincia': 'AZUAY', 'region': 'SIERRA'},\n",
        "            '10927': {'canton': 'SANTA ISABEL', 'provincia': 'AZUAY', 'region': 'SIERRA'},\n",
        "            '20606': {'canton': 'PLAYAS', 'provincia': 'GUAYAS', 'region': 'COSTA'},\n",
        "            '21101': {'canton': 'GUAYAQUIL', 'provincia': 'GUAYAS', 'region': 'COSTA'},\n",
        "            '21709': {'canton': 'MILAGRO', 'provincia': 'GUAYAS', 'region': 'COSTA'},\n",
        "            '31905': {'canton': 'ZAMORA', 'provincia': 'ZAMORA CHINCHIPE', 'region': 'AMAZONIA'},\n",
        "            '20501': {'canton': 'QUITO', 'provincia': 'PICHINCHA', 'region': 'SIERRA'},\n",
        "            '20505': {'canton': 'CAYAMBE', 'provincia': 'PICHINCHA', 'region': 'SIERRA'},\n",
        "            '30101': {'canton': 'LAGO AGRIO', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30201': {'canton': 'GONZALO PIZARRO', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30301': {'canton': 'PUTUMAYO', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30401': {'canton': 'SHUSHUFINDI', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30501': {'canton': 'SUCUMBIOS', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30601': {'canton': 'CASCALES', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "            '30701': {'canton': 'CUYABENO', 'provincia': 'SUCUMBIOS', 'region': 'AMAZONIA'},\n",
        "        }\n",
        "\n",
        "        # Verificar si la columna CANTON existe\n",
        "        col_canton = None\n",
        "        for col in ['CANTON', 'CANT√ìN', 'canton', 'cant√≥n']:\n",
        "            if col in df.columns:\n",
        "                col_canton = col\n",
        "                break\n",
        "\n",
        "        if col_canton is None:\n",
        "            logging.warning(\"No se encontr√≥ columna de cant√≥n. Usando ubicaci√≥n gen√©rica.\")\n",
        "            # Crear una ubicaci√≥n por defecto\n",
        "            dim_ubicacion = pd.DataFrame([{\n",
        "                'ID_Ubicacion': 1,\n",
        "                'CodigoCanton': '99999',\n",
        "                'NombreCanton': 'NO_ESPECIFICADO',\n",
        "                'Provincia': 'NO_ESPECIFICADA',\n",
        "                'Region': 'NO_ESPECIFICADA',\n",
        "                'Pais': 'ECUADOR'\n",
        "            }])\n",
        "        else:\n",
        "            # Obtener cantones √∫nicos del dataset\n",
        "            cantones_dataset = df[col_canton].dropna().unique()\n",
        "\n",
        "            # Crear dimensi√≥n ubicaci√≥n\n",
        "            ubicaciones = []\n",
        "            id_counter = 1\n",
        "\n",
        "            for codigo_canton in cantones_dataset:\n",
        "                codigo_str = str(codigo_canton).strip()\n",
        "                if codigo_str in mapeo_cantones:\n",
        "                    info = mapeo_cantones[codigo_str]\n",
        "                    ubicaciones.append({\n",
        "                        'ID_Ubicacion': id_counter,\n",
        "                        'CodigoCanton': codigo_str,\n",
        "                        'NombreCanton': info['canton'],\n",
        "                        'Provincia': info['provincia'],\n",
        "                        'Region': info['region'],\n",
        "                        'Pais': 'ECUADOR'\n",
        "                    })\n",
        "                else:\n",
        "                    # Para cantones no mapeados, crear entrada gen√©rica\n",
        "                    ubicaciones.append({\n",
        "                        'ID_Ubicacion': id_counter,\n",
        "                        'CodigoCanton': codigo_str,\n",
        "                        'NombreCanton': f'CANTON_{codigo_str}',\n",
        "                        'Provincia': 'NO_IDENTIFICADA',\n",
        "                        'Region': 'NO_IDENTIFICADA',\n",
        "                        'Pais': 'ECUADOR'\n",
        "                    })\n",
        "                id_counter += 1\n",
        "\n",
        "            dim_ubicacion = pd.DataFrame(ubicaciones)\n",
        "\n",
        "        logging.info(f\"üîß Transformaci√≥n completada: {len(dim_ubicacion)} ubicaciones √∫nicas\")\n",
        "\n",
        "        # Cargar a BigQuery\n",
        "        table_id = f'{PROJECT_ID}.{DATASET_ID}.dim_ubicacion'\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "        job = bigquery_client.load_table_from_dataframe(dim_ubicacion, table_id, job_config=job_config)\n",
        "        job.result()\n",
        "\n",
        "        logging.info(f\"‚úÖ Cargados {len(dim_ubicacion)} registros en dim_ubicacion\")\n",
        "        return f\"Dim_Ubicacion cargada exitosamente: {len(dim_ubicacion)} registros\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Error en ETL Dim_Ubicacion: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ===============================\n",
        "# FUNCI√ìN ETL PARA TABLA DE HECHOS\n",
        "# ===============================\n",
        "\n",
        "def etl_fact_registro_vehiculos(**context):\n",
        "    \"\"\"\n",
        "    Proceso ETL para la tabla de hechos Fact_RegistroVehiculos\n",
        "    Realiza lookups con las dimensiones y carga m√©tricas\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"üìä Iniciando ETL para Fact_RegistroVehiculos...\")\n",
        "\n",
        "        # Configurar clientes\n",
        "        storage_client = storage.Client()\n",
        "        bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Extraer datos principales del bucket\n",
        "        bucket = storage_client.bucket(BUCKET_NAME)\n",
        "        blob = bucket.blob('raw-data/sri_vehiculos.csv')\n",
        "\n",
        "        content = blob.download_as_text()\n",
        "        df_hechos = pd.read_csv(StringIO(content))\n",
        "\n",
        "        logging.info(f\"üìä Datos extra√≠dos: {len(df_hechos)} registros de hechos\")\n",
        "\n",
        "        # Cargar dimensiones desde BigQuery para lookups\n",
        "        logging.info(\"üîç Cargando dimensiones para lookups...\")\n",
        "\n",
        "        try:\n",
        "            # Cargar Dim_Tiempo\n",
        "            query_tiempo = f\"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.dim_tiempo`\"\n",
        "            dim_tiempo = bigquery_client.query(query_tiempo).to_dataframe()\n",
        "\n",
        "            # Cargar Dim_Vehiculo\n",
        "            query_vehiculo = f\"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.dim_vehiculo`\"\n",
        "            dim_vehiculo = bigquery_client.query(query_vehiculo).to_dataframe()\n",
        "\n",
        "            # Cargar Dim_Transaccion\n",
        "            query_transaccion = f\"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.dim_transaccion`\"\n",
        "            dim_transaccion = bigquery_client.query(query_transaccion).to_dataframe()\n",
        "\n",
        "            # Cargar Dim_Ubicacion\n",
        "            query_ubicacion = f\"SELECT * FROM `{PROJECT_ID}.{DATASET_ID}.dim_ubicacion`\"\n",
        "            dim_ubicacion = bigquery_client.query(query_ubicacion).to_dataframe()\n",
        "\n",
        "            logging.info(\"‚úÖ Dimensiones cargadas para lookups\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error cargando dimensiones: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # Procesamiento de fechas\n",
        "        logging.info(\"üìÖ Procesando fechas...\")\n",
        "\n",
        "        # Buscar columna de fecha\n",
        "        col_fecha = None\n",
        "        for col in ['FECHA PROCESO', 'FECHA_PROCESO', 'fecha_proceso', 'FECHA']:\n",
        "            if col in df_hechos.columns:\n",
        "                col_fecha = col\n",
        "                break\n",
        "\n",
        "        if col_fecha:\n",
        "            try:\n",
        "                df_hechos['FECHA_PROCESO_CONV'] = pd.to_datetime(df_hechos[col_fecha], errors='coerce')\n",
        "                # Filtrar fechas v√°lidas\n",
        "                df_hechos = df_hechos.dropna(subset=['FECHA_PROCESO_CONV'])\n",
        "                df_hechos['FECHA_PROCESO_DATE'] = df_hechos['FECHA_PROCESO_CONV'].dt.date\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Error procesando fechas: {str(e)}. Usando fecha por defecto.\")\n",
        "                df_hechos['FECHA_PROCESO_DATE'] = datetime.now().date()\n",
        "        else:\n",
        "            logging.warning(\"No se encontr√≥ columna de fecha. Usando fecha actual.\")\n",
        "            df_hechos['FECHA_PROCESO_DATE'] = datetime.now().date()\n",
        "\n",
        "        # Realizar lookups con dimensiones\n",
        "        logging.info(\"üîó Realizando lookups con dimensiones...\")\n",
        "\n",
        "        # Lookup con Dim_Tiempo\n",
        "        df_hechos = df_hechos.merge(\n",
        "            dim_tiempo[['ID_Tiempo', 'FechaCompleta']],\n",
        "            left_on='FECHA_PROCESO_DATE',\n",
        "            right_on='FechaCompleta',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Lookup con Dim_Vehiculo (usando c√≥digo de veh√≠culo)\n",
        "        col_codigo_vehiculo = None\n",
        "        for col in ['C√ìDIGO DE VEH√çCULO', 'CODIGO_VEHICULO', 'codigo_vehiculo']:\n",
        "            if col in df_hechos.columns:\n",
        "                col_codigo_vehiculo = col\n",
        "                break\n",
        "\n",
        "        if col_codigo_vehiculo:\n",
        "            df_hechos = df_hechos.merge(\n",
        "                dim_vehiculo[['ID_Vehiculo', 'CodigoVehiculo']],\n",
        "                left_on=col_codigo_vehiculo,\n",
        "                right_on='CodigoVehiculo',\n",
        "                how='left'\n",
        "            )\n",
        "        else:\n",
        "            df_hechos['ID_Vehiculo'] = 1  # ID por defecto\n",
        "\n",
        "        # Lookup con Dim_Transaccion\n",
        "        merge_cols = []\n",
        "        if 'TIPO TRANSACCI√ìN' in df_hechos.columns and 'TipoTransaccion' in dim_transaccion.columns:\n",
        "            merge_cols.append(('TIPO TRANSACCI√ìN', 'TipoTransaccion'))\n",
        "        if 'TIPO SERVICIO' in df_hechos.columns and 'TipoServicio' in dim_transaccion.columns:\n",
        "            merge_cols.append(('TIPO SERVICIO', 'TipoServicio'))\n",
        "\n",
        "        if merge_cols:\n",
        "            left_cols = [col[0] for col in merge_cols]\n",
        "            right_cols = [col[1] for col in merge_cols]\n",
        "            df_hechos = df_hechos.merge(\n",
        "                dim_transaccion[['ID_Transaccion'] + right_cols],\n",
        "                left_on=left_cols,\n",
        "                right_on=right_cols,\n",
        "                how='left'\n",
        "            )\n",
        "        else:\n",
        "            df_hechos['ID_Transaccion'] = 1  # ID por defecto\n",
        "\n",
        "        # Lookup con Dim_Ubicacion\n",
        "        col_canton = None\n",
        "        for col in ['CANTON', 'CANT√ìN', 'canton']:\n",
        "            if col in df_hechos.columns:\n",
        "                col_canton = col\n",
        "                break\n",
        "\n",
        "        if col_canton:\n",
        "            df_hechos[col_canton] = df_hechos[col_canton].astype(str)\n",
        "            df_hechos = df_hechos.merge(\n",
        "                dim_ubicacion[['ID_Ubicacion', 'CodigoCanton']],\n",
        "                left_on=col_canton,\n",
        "                right_on='CodigoCanton',\n",
        "                how='left'\n",
        "            )\n",
        "        else:\n",
        "            df_hechos['ID_Ubicacion'] = 1  # ID por defecto\n",
        "\n",
        "        # Crear tabla de hechos final\n",
        "        logging.info(\"üìã Creando tabla de hechos final...\")\n",
        "\n",
        "        # Generar ID √∫nico para cada registro\n",
        "        df_hechos['ID_Registro'] = range(1, len(df_hechos) + 1)\n",
        "\n",
        "        # Calcular m√©tricas\n",
        "        df_hechos['CantidadRegistros'] = 1\n",
        "\n",
        "        # Buscar columna de aval√∫o\n",
        "        col_avaluo = None\n",
        "        for col in ['AVALUO', 'AVAL√öO', 'avaluo', 'aval√∫o']:\n",
        "            if col in df_hechos.columns:\n",
        "                col_avaluo = col\n",
        "                break\n",
        "\n",
        "        if col_avaluo:\n",
        "            df_hechos['MontoAvaluo'] = pd.to_numeric(df_hechos[col_avaluo], errors='coerce').fillna(0)\n",
        "        else:\n",
        "            df_hechos['MontoAvaluo'] = 0\n",
        "\n",
        "        # Seleccionar columnas finales para la tabla de hechos\n",
        "        columnas_fact = [\n",
        "            'ID_Registro',\n",
        "            'ID_Tiempo',\n",
        "            'ID_Vehiculo',\n",
        "            'ID_Transaccion',\n",
        "            'ID_Ubicacion',\n",
        "            'CantidadRegistros',\n",
        "            'MontoAvaluo'\n",
        "        ]\n",
        "\n",
        "        # Verificar que todas las columnas existen\n",
        "        columnas_existentes = [col for col in columnas_fact if col in df_hechos.columns]\n",
        "        fact_table = df_hechos[columnas_existentes].copy()\n",
        "\n",
        "        # Llenar valores nulos con defaults\n",
        "        for col in ['ID_Tiempo', 'ID_Vehiculo', 'ID_Transaccion', 'ID_Ubicacion']:\n",
        "            if col in fact_table.columns:\n",
        "                fact_table[col] = fact_table[col].fillna(1)\n",
        "\n",
        "        fact_table = fact_table.fillna(0)\n",
        "\n",
        "        logging.info(f\"üîß Tabla de hechos creada: {len(fact_table)} registros\")\n",
        "\n",
        "        # Cargar a BigQuery\n",
        "        table_id = f'{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos'\n",
        "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "        job = bigquery_client.load_table_from_dataframe(fact_table, table_id, job_config=job_config)\n",
        "        job.result()\n",
        "\n",
        "        logging.info(f\"‚úÖ Cargados {len(fact_table)} registros en fact_registro_vehiculos\")\n",
        "        return f\"Fact_RegistroVehiculos cargada exitosamente: {len(fact_table)} registros\"\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Error en ETL Fact_RegistroVehiculos: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ===============================\n",
        "# DEFINICI√ìN DE TAREAS DEL DAG\n",
        "# ===============================\n",
        "\n",
        "# Tarea de inicio\n",
        "inicio = DummyOperator(\n",
        "    task_id='inicio_proceso_etl',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Tareas ETL para dimensiones\n",
        "tarea_dim_tiempo = PythonOperator(\n",
        "    task_id='etl_dim_tiempo',\n",
        "    python_callable=etl_dim_tiempo,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "tarea_dim_vehiculo = PythonOperator(\n",
        "    task_id='etl_dim_vehiculo',\n",
        "    python_callable=etl_dim_vehiculo,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "tarea_dim_transaccion = PythonOperator(\n",
        "    task_id='etl_dim_transaccion',\n",
        "    python_callable=etl_dim_transaccion,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "tarea_dim_ubicacion = PythonOperator(\n",
        "    task_id='etl_dim_ubicacion',\n",
        "    python_callable=etl_dim_ubicacion,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Tarea de sincronizaci√≥n para dimensiones\n",
        "sincronizacion_dimensiones = DummyOperator(\n",
        "    task_id='sincronizacion_dimensiones',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Tarea ETL para tabla de hechos\n",
        "tarea_fact_registro = PythonOperator(\n",
        "    task_id='etl_fact_registro_vehiculos',\n",
        "    python_callable=etl_fact_registro_vehiculos,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# Tarea de finalizaci√≥n\n",
        "finalizacion = DummyOperator(\n",
        "    task_id='finalizacion_proceso_etl',\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# FUNCIONES DE VALIDACI√ìN Y MONITOREO\n",
        "# ===============================\n",
        "\n",
        "def validar_calidad_datos(**context):\n",
        "    \"\"\"\n",
        "    Funci√≥n para validar la calidad de los datos cargados\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"üîç Iniciando validaci√≥n de calidad de datos...\")\n",
        "\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # Validaciones para dimensiones\n",
        "        validaciones = []\n",
        "\n",
        "        # Validar Dim_Tiempo\n",
        "        query_tiempo = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_registros,\n",
        "            COUNT(DISTINCT Anio) as anios_unicos,\n",
        "            MIN(FechaCompleta) as fecha_min,\n",
        "            MAX(FechaCompleta) as fecha_max\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.dim_tiempo`\n",
        "        \"\"\"\n",
        "\n",
        "        result_tiempo = client.query(query_tiempo).to_dataframe()\n",
        "        validaciones.append(f\"Dim_Tiempo: {result_tiempo.iloc[0]['total_registros']} registros, \"\n",
        "                          f\"a√±os {result_tiempo.iloc[0]['anios_unicos']}, \"\n",
        "                          f\"rango: {result_tiempo.iloc[0]['fecha_min']} a {result_tiempo.iloc[0]['fecha_max']}\")\n",
        "\n",
        "        # Validar Dim_Vehiculo\n",
        "        query_vehiculo = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_registros,\n",
        "            COUNT(DISTINCT Marca) as marcas_unicas,\n",
        "            COUNT(DISTINCT Clase) as clases_unicas\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.dim_vehiculo`\n",
        "        \"\"\"\n",
        "\n",
        "        result_vehiculo = client.query(query_vehiculo).to_dataframe()\n",
        "        validaciones.append(f\"Dim_Vehiculo: {result_vehiculo.iloc[0]['total_registros']} registros, \"\n",
        "                          f\"{result_vehiculo.iloc[0]['marcas_unicas']} marcas, \"\n",
        "                          f\"{result_vehiculo.iloc[0]['clases_unicas']} clases\")\n",
        "\n",
        "        # Validar Dim_Transaccion\n",
        "        query_transaccion = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_registros,\n",
        "            COUNT(DISTINCT TipoTransaccion) as tipos_transaccion\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.dim_transaccion`\n",
        "        \"\"\"\n",
        "\n",
        "        result_transaccion = client.query(query_transaccion).to_dataframe()\n",
        "        validaciones.append(f\"Dim_Transaccion: {result_transaccion.iloc[0]['total_registros']} registros, \"\n",
        "                          f\"{result_transaccion.iloc[0]['tipos_transaccion']} tipos de transacci√≥n\")\n",
        "\n",
        "        # Validar Dim_Ubicacion\n",
        "        query_ubicacion = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_registros,\n",
        "            COUNT(DISTINCT Provincia) as provincias_unicas,\n",
        "            COUNT(DISTINCT Region) as regiones_unicas\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.dim_ubicacion`\n",
        "        \"\"\"\n",
        "\n",
        "        result_ubicacion = client.query(query_ubicacion).to_dataframe()\n",
        "        validaciones.append(f\"Dim_Ubicacion: {result_ubicacion.iloc[0]['total_registros']} registros, \"\n",
        "                          f\"{result_ubicacion.iloc[0]['provincias_unicas']} provincias, \"\n",
        "                          f\"{result_ubicacion.iloc[0]['regiones_unicas']} regiones\")\n",
        "\n",
        "        # Validar Fact_RegistroVehiculos\n",
        "        query_fact = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as total_registros,\n",
        "            SUM(CantidadRegistros) as total_cantidad,\n",
        "            AVG(MontoAvaluo) as avaluo_promedio,\n",
        "            COUNT(CASE WHEN ID_Tiempo IS NULL THEN 1 END) as registros_sin_tiempo,\n",
        "            COUNT(CASE WHEN ID_Vehiculo IS NULL THEN 1 END) as registros_sin_vehiculo\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos`\n",
        "        \"\"\"\n",
        "\n",
        "        result_fact = client.query(query_fact).to_dataframe()\n",
        "        validaciones.append(f\"Fact_RegistroVehiculos: {result_fact.iloc[0]['total_registros']} registros, \"\n",
        "                          f\"cantidad total: {result_fact.iloc[0]['total_cantidad']}, \"\n",
        "                          f\"aval√∫o promedio: ${result_fact.iloc[0]['avaluo_promedio']:,.2f}\")\n",
        "\n",
        "        # Log de todas las validaciones\n",
        "        for validacion in validaciones:\n",
        "            logging.info(f\"‚úÖ {validacion}\")\n",
        "\n",
        "        # Verificar integridad referencial\n",
        "        query_integridad = f\"\"\"\n",
        "        SELECT\n",
        "            COUNT(*) as registros_con_claves_validas\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos` f\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_tiempo` t ON f.ID_Tiempo = t.ID_Tiempo\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_vehiculo` v ON f.ID_Vehiculo = v.ID_Vehiculo\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_transaccion` tr ON f.ID_Transaccion = tr.ID_Transaccion\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_ubicacion` u ON f.ID_Ubicacion = u.ID_Ubicacion\n",
        "        \"\"\"\n",
        "\n",
        "        result_integridad = client.query(query_integridad).to_dataframe()\n",
        "        registros_validos = result_integridad.iloc[0]['registros_con_claves_validas']\n",
        "\n",
        "        logging.info(f\"üîó Integridad referencial: {registros_validos} registros con todas las claves v√°lidas\")\n",
        "\n",
        "        resumen_validacion = {\n",
        "            'validaciones': validaciones,\n",
        "            'registros_con_integridad': registros_validos,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return resumen_validacion\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Error en validaci√≥n de calidad: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def generar_metricas_negocio(**context):\n",
        "    \"\"\"\n",
        "    Genera m√©tricas de negocio del proceso ETL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"üìà Generando m√©tricas de negocio...\")\n",
        "\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        # M√©tricas por a√±o\n",
        "        query_por_anio = f\"\"\"\n",
        "        SELECT\n",
        "            t.Anio,\n",
        "            COUNT(*) as total_registros,\n",
        "            SUM(f.MontoAvaluo) as monto_total_avaluo,\n",
        "            AVG(f.MontoAvaluo) as monto_promedio_avaluo\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos` f\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_tiempo` t ON f.ID_Tiempo = t.ID_Tiempo\n",
        "        GROUP BY t.Anio\n",
        "        ORDER BY t.Anio DESC\n",
        "        LIMIT 5\n",
        "        \"\"\"\n",
        "\n",
        "        metricas_anio = client.query(query_por_anio).to_dataframe()\n",
        "\n",
        "        # M√©tricas por marca\n",
        "        query_por_marca = f\"\"\"\n",
        "        SELECT\n",
        "            v.Marca,\n",
        "            COUNT(*) as total_registros,\n",
        "            AVG(f.MontoAvaluo) as avaluo_promedio\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos` f\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_vehiculo` v ON f.ID_Vehiculo = v.ID_Vehiculo\n",
        "        GROUP BY v.Marca\n",
        "        ORDER BY total_registros DESC\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "\n",
        "        metricas_marca = client.query(query_por_marca).to_dataframe()\n",
        "\n",
        "        # M√©tricas por provincia\n",
        "        query_por_provincia = f\"\"\"\n",
        "        SELECT\n",
        "            u.Provincia,\n",
        "            u.Region,\n",
        "            COUNT(*) as total_registros,\n",
        "            SUM(f.MontoAvaluo) as monto_total\n",
        "        FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos` f\n",
        "        INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_ubicacion` u ON f.ID_Ubicacion = u.ID_Ubicacion\n",
        "        GROUP BY u.Provincia, u.Region\n",
        "        ORDER BY total_registros DESC\n",
        "        LIMIT 10\n",
        "        \"\"\"\n",
        "\n",
        "        metricas_provincia = client.query(query_por_provincia).to_dataframe()\n",
        "\n",
        "        # Log de m√©tricas\n",
        "        logging.info(\"üìä M√âTRICAS POR A√ëO:\")\n",
        "        for _, row in metricas_anio.iterrows():\n",
        "            logging.info(f\"   {row['Anio']}: {row['total_registros']} registros, \"\n",
        "                        f\"aval√∫o total: ${row['monto_total_avaluo']:,.2f}\")\n",
        "\n",
        "        logging.info(\"üöó TOP MARCAS:\")\n",
        "        for _, row in metricas_marca.iterrows():\n",
        "            logging.info(f\"   {row['Marca']}: {row['total_registros']} registros, \"\n",
        "                        f\"aval√∫o promedio: ${row['avaluo_promedio']:,.2f}\")\n",
        "\n",
        "        logging.info(\"üåé TOP PROVINCIAS:\")\n",
        "        for _, row in metricas_provincia.iterrows():\n",
        "            logging.info(f\"   {row['Provincia']} ({row['Region']}): {row['total_registros']} registros\")\n",
        "\n",
        "        metricas_resumen = {\n",
        "            'metricas_por_anio': metricas_anio.to_dict('records'),\n",
        "            'metricas_por_marca': metricas_marca.to_dict('records'),\n",
        "            'metricas_por_provincia': metricas_provincia.to_dict('records'),\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        return metricas_resumen\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Error generando m√©tricas: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def notificar_finalizacion(**context):\n",
        "    \"\"\"\n",
        "    Notifica la finalizaci√≥n exitosa del proceso ETL\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logging.info(\"üìß Enviando notificaci√≥n de finalizaci√≥n...\")\n",
        "\n",
        "        # Obtener informaci√≥n del contexto\n",
        "        dag_run = context['dag_run']\n",
        "        execution_date = context['execution_date']\n",
        "\n",
        "        # Crear resumen del proceso\n",
        "        resumen = {\n",
        "            'dag_id': dag_run.dag_id,\n",
        "            'execution_date': execution_date.isoformat(),\n",
        "            'estado': 'EXITOSO',\n",
        "            'duracion_total': str(datetime.now() - dag_run.start_date) if dag_run.start_date else 'N/A',\n",
        "            'timestamp_finalizacion': datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        logging.info(\"‚úÖ PROCESO ETL FINALIZADO EXITOSAMENTE\")\n",
        "        logging.info(f\"   DAG: {resumen['dag_id']}\")\n",
        "        logging.info(f\"   Fecha de ejecuci√≥n: {resumen['execution_date']}\")\n",
        "        logging.info(f\"   Duraci√≥n: {resumen['duracion_total']}\")\n",
        "        logging.info(f\"   Estado: {resumen['estado']}\")\n",
        "\n",
        "        # Aqu√≠ se puede agregar l√≥gica para enviar emails, Slack, etc.\n",
        "        # Por ejemplo:\n",
        "        # send_email_notification(resumen)\n",
        "        # send_slack_notification(resumen)\n",
        "\n",
        "        return resumen\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Error en notificaci√≥n: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# ===============================\n",
        "# TAREAS DE VALIDACI√ìN Y MONITOREO\n",
        "# ===============================\n",
        "\n",
        "tarea_validacion = PythonOperator(\n",
        "    task_id='validar_calidad_datos',\n",
        "    python_callable=validar_calidad_datos,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "tarea_metricas = PythonOperator(\n",
        "    task_id='generar_metricas_negocio',\n",
        "    python_callable=generar_metricas_negocio,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "tarea_notificacion = PythonOperator(\n",
        "    task_id='notificar_finalizacion',\n",
        "    python_callable=notificar_finalizacion,\n",
        "    dag=dag\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# DEFINICI√ìN DE DEPENDENCIAS DEL DAG\n",
        "# ===============================\n",
        "\n",
        "# Estructura de dependencias:\n",
        "# inicio -> [dimensiones en paralelo] -> sincronizaci√≥n -> tabla_hechos -> validaci√≥n -> m√©tricas -> notificaci√≥n -> fin\n",
        "\n",
        "# Inicio del proceso\n",
        "inicio >> [tarea_dim_tiempo, tarea_dim_vehiculo, tarea_dim_transaccion, tarea_dim_ubicacion]\n",
        "\n",
        "# Sincronizaci√≥n de dimensiones\n",
        "[tarea_dim_tiempo, tarea_dim_vehiculo, tarea_dim_transaccion, tarea_dim_ubicacion] >> sincronizacion_dimensiones\n",
        "\n",
        "# Tabla de hechos (despu√©s de que todas las dimensiones est√©n listas)\n",
        "sincronizacion_dimensiones >> tarea_fact_registro\n",
        "\n",
        "# Validaci√≥n y m√©tricas\n",
        "tarea_fact_registro >> tarea_validacion >> tarea_metricas >> tarea_notificacion >> finalizacion\n",
        "\n",
        "# ===============================\n",
        "# CONFIGURACI√ìN ADICIONAL DEL DAG\n",
        "# ===============================\n",
        "\n",
        "# Configurar el DAG para logging detallado\n",
        "dag.doc_md = \"\"\"\n",
        "# DAG ETL SRI Veh√≠culos\n",
        "\n",
        "Este DAG implementa un proceso ETL completo para los datos vehiculares del SRI.\n",
        "\n",
        "## Estructura del Proceso:\n",
        "\n",
        "1. **Dimensiones (Paralelo)**:\n",
        "   - `dim_tiempo`: Genera calendario completo 2020-2025\n",
        "   - `dim_vehiculo`: Extrae caracter√≠sticas √∫nicas de veh√≠culos\n",
        "   - `dim_transaccion`: Mapea tipos de transacciones\n",
        "   - `dim_ubicacion`: Mapea c√≥digos de cant√≥n a geograf√≠a\n",
        "\n",
        "2. **Tabla de Hechos**:\n",
        "   - `fact_registro_vehiculos`: Combina todas las dimensiones con m√©tricas\n",
        "\n",
        "3. **Validaci√≥n y Monitoreo**:\n",
        "   - Validaci√≥n de calidad de datos\n",
        "   - Generaci√≥n de m√©tricas de negocio\n",
        "   - Notificaciones de finalizaci√≥n\n",
        "\n",
        "## Configuraci√≥n Requerida:\n",
        "\n",
        "- PROJECT_ID: ID del proyecto de Google Cloud\n",
        "- DATASET_ID: Nombre del dataset en BigQuery\n",
        "- BUCKET_NAME: Nombre del bucket de Cloud Storage\n",
        "- Service Account con permisos de BigQuery y Cloud Storage\n",
        "\n",
        "## Archivos Requeridos:\n",
        "\n",
        "- `gs://[BUCKET_NAME]/raw-data/sri_vehiculos.csv`\n",
        "\n",
        "## Tablas Generadas:\n",
        "\n",
        "- `dim_tiempo`\n",
        "- `dim_vehiculo`\n",
        "- `dim_transaccion`\n",
        "- `dim_ubicacion`\n",
        "- `fact_registro_vehiculos`\n",
        "\"\"\"\n",
        "\n",
        "# Configurar tags adicionales para organizaci√≥n\n",
        "dag.tags.extend(['data-warehouse', 'gobierno', 'vehiculos'])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dag.test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "5Q7j0Q3Ha5v8",
        "outputId": "019dbb53-4e8e-4364-9992-4e98ccf543be"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OperationalError",
          "evalue": "(sqlite3.OperationalError) no such table: task_instance\n[SQL: SELECT task_instance.try_number, task_instance.task_id, task_instance.dag_id, task_instance.run_id, task_instance.map_index, task_instance.start_date, task_instance.end_date, task_instance.duration, task_instance.state, task_instance.max_tries, task_instance.hostname, task_instance.unixname, task_instance.job_id, task_instance.pool, task_instance.pool_slots, task_instance.queue, task_instance.priority_weight, task_instance.operator, task_instance.custom_operator_name, task_instance.queued_dttm, task_instance.queued_by_job_id, task_instance.pid, task_instance.executor_config, task_instance.updated_at, task_instance.external_executor_id, task_instance.trigger_id, task_instance.trigger_timeout, task_instance.next_method, task_instance.next_kwargs, dag_run_1.state AS state_1, dag_run_1.id, dag_run_1.dag_id AS dag_id_1, dag_run_1.queued_at, dag_run_1.execution_date, dag_run_1.start_date AS start_date_1, dag_run_1.end_date AS end_date_1, dag_run_1.run_id AS run_id_1, dag_run_1.creating_job_id, dag_run_1.external_trigger, dag_run_1.run_type, dag_run_1.conf, dag_run_1.data_interval_start, dag_run_1.data_interval_end, dag_run_1.last_scheduling_decision, dag_run_1.dag_hash, dag_run_1.log_template_id, dag_run_1.updated_at AS updated_at_1 \nFROM task_instance JOIN dag_run ON dag_run.dag_id = task_instance.dag_id AND dag_run.run_id = task_instance.run_id JOIN dag_run AS dag_run_1 ON dag_run_1.dag_id = task_instance.dag_id AND dag_run_1.run_id = task_instance.run_id \nWHERE task_instance.dag_id = ? AND task_instance.task_id IN (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?) AND dag_run.execution_date >= ? AND dag_run.execution_date <= ? AND task_instance.operator = ?]\n[parameters: ('sri_vehiculos_etl_proceso', 'inicio_proceso_etl', 'etl_dim_tiempo', 'etl_dim_vehiculo', 'etl_dim_transaccion', 'etl_dim_ubicacion', 'sincronizacion_dimensiones', 'etl_fact_registro_vehiculos', 'finalizacion_proceso_etl', 'validar_calidad_datos', 'generar_metricas_negocio', 'notificar_finalizacion', '2025-07-02 05:07:54.590300', '2025-07-02 05:07:54.590300', 'ExternalTaskMarker')]\n(Background on this error at: https://sqlalche.me/e/14/e3q8)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1909\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m                     self.dialect.do_execute(\n\u001b[0m\u001b[1;32m   1911\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: no such table: task_instance",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35-949037698.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m     \u001b[0mdag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/airflow/utils/session.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/airflow/models/dag.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, execution_date, run_conf, conn_file_path, variable_file_path, session)\u001b[0m\n\u001b[1;32m   2718\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2719\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clearing existing task instances for execution date %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2720\u001b[0;31m         self.clear(\n\u001b[0m\u001b[1;32m   2721\u001b[0m             \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m             \u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/airflow/utils/session.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"session\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msession_args_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcreate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/airflow/models/dag.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self, task_ids, start_date, end_date, only_failed, only_running, confirm_prompt, include_subdags, include_parentdag, dag_run_state, dry_run, session, get_tis, recursion_depth, max_recursion_depth, dag_bag, exclude_task_ids)\u001b[0m\n\u001b[1;32m   2195\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTaskInstanceState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m         tis = self._get_task_instances(\n\u001b[0m\u001b[1;32m   2198\u001b[0m             \u001b[0mtask_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m             \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/airflow/models/dag.py\u001b[0m in \u001b[0;36m_get_task_instances\u001b[0;34m(self, task_ids, as_pk_tuple, start_date, end_date, run_id, state, include_subdags, include_parentdag, include_dependent_dags, exclude_task_ids, session, dag_bag, recursion_depth, max_recursion_depth, visited_external_tis)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mvisited_external_tis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0mexternal_tasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mExternalTaskMarker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mti\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexternal_tasks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36mscalars\u001b[0;34m(self, statement, params, execution_options, bind_arguments, **kw)\u001b[0m\n\u001b[1;32m   1776\u001b[0m         \"\"\"\n\u001b[1;32m   1777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m         return self.execute(\n\u001b[0m\u001b[1;32m   1779\u001b[0m             \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/orm/session.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, statement, params, execution_options, bind_arguments, _parent_execute_state, _add_event, **kw)\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m             \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection_for_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_20\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompile_state_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_20\u001b[0;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[1;32m   1708\u001b[0m             )\n\u001b[1;32m   1709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_10style\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_10style\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m     def exec_driver_sql(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, multiparams, params, execution_options, _force)\u001b[0m\n\u001b[1;32m    332\u001b[0m     ):\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_force\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_execution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             return connection._execute_clauseelement(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, multiparams, params, execution_options)\u001b[0m\n\u001b[1;32m   1575\u001b[0m             \u001b[0mlinting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiler_linting\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcompiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN_LINTING\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m         )\n\u001b[0;32m-> 1577\u001b[0;31m         ret = self._execute_context(\n\u001b[0m\u001b[1;32m   1578\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_ctx_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_compiled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m             self._handle_dbapi_exception(\n\u001b[0m\u001b[1;32m   1954\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   2132\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewraise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2133\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2134\u001b[0;31m                 util.raise_(\n\u001b[0m\u001b[1;32m   2135\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2136\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;31m# credit to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[1;32m   1908\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m                     self.dialect.do_execute(\n\u001b[0m\u001b[1;32m   1911\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sqlalchemy/engine/default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m         \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such table: task_instance\n[SQL: SELECT task_instance.try_number, task_instance.task_id, task_instance.dag_id, task_instance.run_id, task_instance.map_index, task_instance.start_date, task_instance.end_date, task_instance.duration, task_instance.state, task_instance.max_tries, task_instance.hostname, task_instance.unixname, task_instance.job_id, task_instance.pool, task_instance.pool_slots, task_instance.queue, task_instance.priority_weight, task_instance.operator, task_instance.custom_operator_name, task_instance.queued_dttm, task_instance.queued_by_job_id, task_instance.pid, task_instance.executor_config, task_instance.updated_at, task_instance.external_executor_id, task_instance.trigger_id, task_instance.trigger_timeout, task_instance.next_method, task_instance.next_kwargs, dag_run_1.state AS state_1, dag_run_1.id, dag_run_1.dag_id AS dag_id_1, dag_run_1.queued_at, dag_run_1.execution_date, dag_run_1.start_date AS start_date_1, dag_run_1.end_date AS end_date_1, dag_run_1.run_id AS run_id_1, dag_run_1.creating_job_id, dag_run_1.external_trigger, dag_run_1.run_type, dag_run_1.conf, dag_run_1.data_interval_start, dag_run_1.data_interval_end, dag_run_1.last_scheduling_decision, dag_run_1.dag_hash, dag_run_1.log_template_id, dag_run_1.updated_at AS updated_at_1 \nFROM task_instance JOIN dag_run ON dag_run.dag_id = task_instance.dag_id AND dag_run.run_id = task_instance.run_id JOIN dag_run AS dag_run_1 ON dag_run_1.dag_id = task_instance.dag_id AND dag_run_1.run_id = task_instance.run_id \nWHERE task_instance.dag_id = ? AND task_instance.task_id IN (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?) AND dag_run.execution_date >= ? AND dag_run.execution_date <= ? AND task_instance.operator = ?]\n[parameters: ('sri_vehiculos_etl_proceso', 'inicio_proceso_etl', 'etl_dim_tiempo', 'etl_dim_vehiculo', 'etl_dim_transaccion', 'etl_dim_ubicacion', 'sincronizacion_dimensiones', 'etl_fact_registro_vehiculos', 'finalizacion_proceso_etl', 'validar_calidad_datos', 'generar_metricas_negocio', 'notificar_finalizacion', '2025-07-02 05:07:54.590300', '2025-07-02 05:07:54.590300', 'ExternalTaskMarker')]\n(Background on this error at: https://sqlalche.me/e/14/e3q8)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 9: Ejecutar ETL de dimensiones\n",
        "print(\"üöÄ Ejecutando ETL de dimensiones...\")\n",
        "\n",
        "# Ejecutar cada dimensi√≥n\n",
        "try:\n",
        "    resultado_tiempo = etl_dim_tiempo()\n",
        "    print(f\"1Ô∏è‚É£ {resultado_tiempo}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en dim_tiempo: {e}\")\n",
        "\n",
        "try:\n",
        "    resultado_vehiculo = etl_dim_vehiculo()\n",
        "    print(f\"2Ô∏è‚É£ {resultado_vehiculo}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en dim_vehiculo: {e}\")\n",
        "\n",
        "try:\n",
        "    resultado_transaccion = etl_dim_transaccion()\n",
        "    print(f\"3Ô∏è‚É£ {resultado_transaccion}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en dim_transaccion: {e}\")\n",
        "\n",
        "try:\n",
        "    resultado_ubicacion = etl_dim_ubicacion()\n",
        "    print(f\"4Ô∏è‚É£ {resultado_ubicacion}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en dim_ubicacion: {e}\")\n",
        "\n",
        "print(\"‚úÖ Dimensiones completadas\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awDo5jvvdzUu",
        "outputId": "7c19c064-8214-4996-c289-a7e6c036acf4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Ejecutando ETL de dimensiones...\n",
            "[\u001b[34m2025-07-02T05:09:08.525+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m56} INFO\u001b[0m - üïê Iniciando ETL para Dim_Tiempo...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:08.648+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m66} INFO\u001b[0m - üìÖ Generando 2192 registros de fechas...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:15.092+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m116} INFO\u001b[0m - ‚úÖ Cargados 2192 registros en dim_tiempo\u001b[0m\n",
            "1Ô∏è‚É£ Dim_Tiempo cargada exitosamente: 2192 registros\n",
            "[\u001b[34m2025-07-02T05:09:15.094+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m129} INFO\u001b[0m - üöó Iniciando ETL para Dim_Vehiculo...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:19.223+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m143} INFO\u001b[0m - üìä Datos extra√≠dos: 460550 registros originales\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:21.910+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m196} INFO\u001b[0m - üîß Transformaci√≥n completada: 331160 veh√≠culos √∫nicos\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:27.789+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m205} INFO\u001b[0m - ‚úÖ Cargados 331160 registros en dim_vehiculo\u001b[0m\n",
            "2Ô∏è‚É£ Dim_Vehiculo cargada exitosamente: 331160 registros\n",
            "[\u001b[34m2025-07-02T05:09:27.791+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m218} INFO\u001b[0m - üíº Iniciando ETL para Dim_Transaccion...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:30.988+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m239} INFO\u001b[0m - Columnas encontradas: ['TIPO TRANSACCI√ìN', 'TIPO SERVICIO', 'PERSONA NATURAL - JUR√çDICA', 'CATEGOR√çA']\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:31.122+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m267} INFO\u001b[0m - üîß Transformaci√≥n completada: 8848 tipos de transacci√≥n √∫nicos\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:36.345+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m276} INFO\u001b[0m - ‚úÖ Cargados 8848 registros en dim_transaccion\u001b[0m\n",
            "3Ô∏è‚É£ Dim_Transaccion cargada exitosamente: 8848 registros\n",
            "[\u001b[34m2025-07-02T05:09:36.350+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m289} INFO\u001b[0m - üåé Iniciando ETL para Dim_Ubicacion...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:39.482+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m375} INFO\u001b[0m - üîß Transformaci√≥n completada: 200 ubicaciones √∫nicas\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:41.824+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m384} INFO\u001b[0m - ‚úÖ Cargados 200 registros en dim_ubicacion\u001b[0m\n",
            "4Ô∏è‚É£ Dim_Ubicacion cargada exitosamente: 200 registros\n",
            "‚úÖ Dimensiones completadas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 10: Ejecutar ETL de tabla de hechos\n",
        "print(\"üìä Ejecutando ETL de tabla de hechos...\")\n",
        "\n",
        "try:\n",
        "    resultado_fact = etl_fact_registro_vehiculos()\n",
        "    print(f\"‚úÖ {resultado_fact}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en fact table: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk_qFGzLin_Y",
        "outputId": "c146fb61-b06e-4849-c042-523bd776ec57"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Ejecutando ETL de tabla de hechos...\n",
            "[\u001b[34m2025-07-02T05:09:49.220+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m401} INFO\u001b[0m - üìä Iniciando ETL para Fact_RegistroVehiculos...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:54.546+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m414} INFO\u001b[0m - üìä Datos extra√≠dos: 460550 registros de hechos\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:54.549+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m417} INFO\u001b[0m - üîç Cargando dimensiones para lookups...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:56.236+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m439} ERROR\u001b[0m - Error cargando dimensiones: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:09:56.239+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m586} ERROR\u001b[0m - ‚ùå Error en ETL Fact_RegistroVehiculos: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\u001b[0m\n",
            "‚ùå Error en fact table: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 11: Ejecutar validaciones y m√©tricas\n",
        "print(\"üîç Ejecutando validaciones...\")\n",
        "\n",
        "try:\n",
        "    validacion = validar_calidad_datos()\n",
        "    print(\"‚úÖ Validaci√≥n completada\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en validaci√≥n: {e}\")\n",
        "\n",
        "try:\n",
        "    metricas = generar_metricas_negocio()\n",
        "    print(\"‚úÖ M√©tricas generadas\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en m√©tricas: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ai-Qz22ixbX",
        "outputId": "32d70a5f-8403-4411-8fd0-a3346ead7cd1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Ejecutando validaciones...\n",
            "[\u001b[34m2025-07-02T05:10:27.669+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m652} INFO\u001b[0m - üîç Iniciando validaci√≥n de calidad de datos...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:10:28.926+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m759} ERROR\u001b[0m - ‚ùå Error en validaci√≥n de calidad: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\u001b[0m\n",
            "‚ùå Error en validaci√≥n: 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n",
            "[\u001b[34m2025-07-02T05:10:28.928+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m767} INFO\u001b[0m - üìà Generando m√©tricas de negocio...\u001b[0m\n",
            "[\u001b[34m2025-07-02T05:10:29.308+0000\u001b[0m] {\u001b[34mipython-input-35-949037698.py:\u001b[0m843} ERROR\u001b[0m - ‚ùå Error generando m√©tricas: 400 Name MontoAvaluo not found inside f at [5:19]; reason: invalidQuery, location: query, message: Name MontoAvaluo not found inside f at [5:19]\n",
            "\n",
            "Location: us-central1\n",
            "Job ID: 9592d064-8692-41c3-bcfd-fdff488b20e1\n",
            "\u001b[0m\n",
            "‚ùå Error en m√©tricas: 400 Name MontoAvaluo not found inside f at [5:19]; reason: invalidQuery, location: query, message: Name MontoAvaluo not found inside f at [5:19]\n",
            "\n",
            "Location: us-central1\n",
            "Job ID: 9592d064-8692-41c3-bcfd-fdff488b20e1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 12: Verificar tablas creadas\n",
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "tablas_esperadas = ['dim_tiempo', 'dim_vehiculo', 'dim_transaccion', 'dim_ubicacion', 'fact_registro_vehiculos']\n",
        "\n",
        "print(\"üìã Verificando tablas creadas:\")\n",
        "for tabla in tablas_esperadas:\n",
        "    try:\n",
        "        query = f\"SELECT COUNT(*) as total FROM `{PROJECT_ID}.{DATASET_ID}.{tabla}`\"\n",
        "        result = client.query(query).to_dataframe()\n",
        "        total = result.iloc[0]['total']\n",
        "        print(f\"‚úÖ {tabla}: {total:,} registros\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {tabla}: Error - {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wzBlRkYi3oW",
        "outputId": "487ca17c-91e3-458e-e8de-1ea1c95564b6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìã Verificando tablas creadas:\n",
            "‚ùå dim_tiempo: Error - 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n",
            "‚ùå dim_vehiculo: Error - 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n",
            "‚ùå dim_transaccion: Error - 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n",
            "‚ùå dim_ubicacion: Error - 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n",
            "‚ùå fact_registro_vehiculos: Error - 403 request failed: the user does not have 'bigquery.readsessions.create' permission for 'projects/sri-vehiculos-etl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda 13: Consulta de ejemplo del data warehouse\n",
        "query_ejemplo = f\"\"\"\n",
        "SELECT\n",
        "    t.Anio,\n",
        "    v.Marca,\n",
        "    u.Provincia,\n",
        "    COUNT(*) as total_registros,\n",
        "    AVG(f.MontoAvaluo) as avaluo_promedio\n",
        "FROM `{PROJECT_ID}.{DATASET_ID}.fact_registro_vehiculos` f\n",
        "INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_tiempo` t ON f.ID_Tiempo = t.ID_Tiempo\n",
        "INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_vehiculo` v ON f.ID_Vehiculo = v.ID_Vehiculo\n",
        "INNER JOIN `{PROJECT_ID}.{DATASET_ID}.dim_ubicacion` u ON f.ID_Ubicacion = u.ID_Ubicacion\n",
        "GROUP BY t.Anio, v.Marca, u.Provincia\n",
        "ORDER BY total_registros DESC\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    resultado = client.query(query_ejemplo).to_dataframe()\n",
        "    print(\"üìä Top 10 combinaciones A√±o-Marca-Provincia:\")\n",
        "    print(resultado.to_string(index=False))\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en consulta: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaVRMh3SjI4e",
        "outputId": "8377c2d5-7140-43f7-a580-4c5c8eb479a9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Error en consulta: 400 Name MontoAvaluo not found inside f at [7:11]; reason: invalidQuery, location: query, message: Name MontoAvaluo not found inside f at [7:11]\n",
            "\n",
            "Location: us-central1\n",
            "Job ID: b0f5d031-ddbd-45b1-bd36-7de68c443629\n",
            "\n"
          ]
        }
      ]
    }
  ]
}